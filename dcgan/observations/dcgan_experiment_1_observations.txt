Hyperparamters

Generator And Discriminator
kernel size 4 > 5 (except last layer of Gen)

Generator
reduced from 5 Conv2dTranspose layers to 3
stride in Conv2dTranspose layers 2 and 3 is 3 instead of 2
changed padding in 2nd and 3rd Conv2dTranspose layers to 0
still outputs shape of n x channels_img x 64 x 64 which is same shape as before


Discriminator Loss



Log
starting training...
Epoch [0/10] Batch 0/938              Loss D: 1.5341, Loss G: 0.5156, D(x): 0.5398
Epoch [0/10] Batch 100/938              Loss D: 1.6159, Loss G: 0.4502, D(x): 0.5470
Epoch [0/10] Batch 200/938              Loss D: 1.5802, Loss G: 0.4842, D(x): 0.5395
Epoch [0/10] Batch 300/938              Loss D: 1.5460, Loss G: 0.5133, D(x): 0.5398
Epoch [0/10] Batch 400/938              Loss D: 1.6027, Loss G: 0.4795, D(x): 0.5270
Epoch [0/10] Batch 500/938              Loss D: 1.5821, Loss G: 0.4894, D(x): 0.5312
Epoch [0/10] Batch 600/938              Loss D: 1.5983, Loss G: 0.4789, D(x): 0.5342
Epoch [0/10] Batch 700/938              Loss D: 1.5242, Loss G: 0.5065, D(x): 0.5549
Epoch [0/10] Batch 800/938              Loss D: 1.5734, Loss G: 0.4893, D(x): 0.5409
Epoch [0/10] Batch 900/938              Loss D: 1.6130, Loss G: 0.4804, D(x): 0.5228
Epoch [1/10] Batch 0/938              Loss D: 1.5605, Loss G: 0.4855, D(x): 0.5472
Epoch [1/10] Batch 100/938              Loss D: 1.6057, Loss G: 0.4695, D(x): 0.5387
Epoch [1/10] Batch 200/938              Loss D: 1.5768, Loss G: 0.4891, D(x): 0.5335
Epoch [1/10] Batch 300/938              Loss D: 1.5524, Loss G: 0.5013, D(x): 0.5463
Epoch [1/10] Batch 400/938              Loss D: 1.5763, Loss G: 0.4789, D(x): 0.5434
Epoch [1/10] Batch 500/938              Loss D: 1.5923, Loss G: 0.4743, D(x): 0.5408
Epoch [1/10] Batch 600/938              Loss D: 1.5566, Loss G: 0.4958, D(x): 0.5420
Epoch [1/10] Batch 700/938              Loss D: 1.5647, Loss G: 0.4921, D(x): 0.5414
Epoch [1/10] Batch 800/938              Loss D: 1.5868, Loss G: 0.4700, D(x): 0.5485
Epoch [1/10] Batch 900/938              Loss D: 1.5907, Loss G: 0.4754, D(x): 0.5487
Epoch [2/10] Batch 0/938              Loss D: 1.5996, Loss G: 0.4820, D(x): 0.5290
Epoch [2/10] Batch 100/938              Loss D: 1.5433, Loss G: 0.5016, D(x): 0.5486
Epoch [2/10] Batch 200/938              Loss D: 1.5878, Loss G: 0.4797, D(x): 0.5345
Epoch [2/10] Batch 300/938              Loss D: 1.5759, Loss G: 0.4843, D(x): 0.5449
Epoch [2/10] Batch 400/938              Loss D: 1.5761, Loss G: 0.4935, D(x): 0.5298
Epoch [2/10] Batch 500/938              Loss D: 1.6186, Loss G: 0.4594, D(x): 0.5331
Epoch [2/10] Batch 600/938              Loss D: 1.6369, Loss G: 0.4638, D(x): 0.5184
Epoch [2/10] Batch 700/938              Loss D: 1.6182, Loss G: 0.4660, D(x): 0.5303
Epoch [2/10] Batch 800/938              Loss D: 1.5642, Loss G: 0.4851, D(x): 0.5432
Epoch [2/10] Batch 900/938              Loss D: 1.5877, Loss G: 0.4787, D(x): 0.5418
Epoch [3/10] Batch 0/938              Loss D: 1.6135, Loss G: 0.4734, D(x): 0.5357
Epoch [3/10] Batch 100/938              Loss D: 1.5841, Loss G: 0.4819, D(x): 0.5373
Epoch [3/10] Batch 200/938              Loss D: 1.6004, Loss G: 0.4558, D(x): 0.5491
Epoch [3/10] Batch 300/938              Loss D: 1.5393, Loss G: 0.5126, D(x): 0.5378
Epoch [3/10] Batch 400/938              Loss D: 1.5739, Loss G: 0.4831, D(x): 0.5408
Epoch [3/10] Batch 500/938              Loss D: 1.5608, Loss G: 0.4913, D(x): 0.5406
Epoch [3/10] Batch 600/938              Loss D: 1.5742, Loss G: 0.4793, D(x): 0.5484
Epoch [3/10] Batch 700/938              Loss D: 1.6132, Loss G: 0.4573, D(x): 0.5402
Epoch [3/10] Batch 800/938              Loss D: 1.5581, Loss G: 0.5003, D(x): 0.5436
Epoch [3/10] Batch 900/938              Loss D: 1.6136, Loss G: 0.4626, D(x): 0.5351
Epoch [4/10] Batch 0/938              Loss D: 1.6059, Loss G: 0.4700, D(x): 0.5351
Epoch [4/10] Batch 100/938              Loss D: 1.6146, Loss G: 0.4730, D(x): 0.5239
Epoch [4/10] Batch 200/938              Loss D: 1.5753, Loss G: 0.4845, D(x): 0.5391
Epoch [4/10] Batch 300/938              Loss D: 1.5622, Loss G: 0.4932, D(x): 0.5440
Epoch [4/10] Batch 400/938              Loss D: 1.5623, Loss G: 0.4976, D(x): 0.5413
Epoch [4/10] Batch 500/938              Loss D: 1.5930, Loss G: 0.4790, D(x): 0.5328
Epoch [4/10] Batch 600/938              Loss D: 1.5654, Loss G: 0.4937, D(x): 0.5415
Epoch [4/10] Batch 700/938              Loss D: 1.5904, Loss G: 0.4804, D(x): 0.5384
Epoch [4/10] Batch 800/938              Loss D: 1.5593, Loss G: 0.4943, D(x): 0.5450
Epoch [4/10] Batch 900/938              Loss D: 1.6077, Loss G: 0.4666, D(x): 0.5378
Epoch [5/10] Batch 0/938              Loss D: 1.5737, Loss G: 0.4741, D(x): 0.5537
Epoch [5/10] Batch 100/938              Loss D: 1.5356, Loss G: 0.5184, D(x): 0.5435
Epoch [5/10] Batch 200/938              Loss D: 1.5215, Loss G: 0.5167, D(x): 0.5477
Epoch [5/10] Batch 300/938              Loss D: 1.5977, Loss G: 0.4840, D(x): 0.5336
Epoch [5/10] Batch 400/938              Loss D: 1.5879, Loss G: 0.4742, D(x): 0.5497
Epoch [5/10] Batch 500/938              Loss D: 1.5649, Loss G: 0.4728, D(x): 0.5568
Epoch [5/10] Batch 600/938              Loss D: 1.5727, Loss G: 0.4850, D(x): 0.5436
Epoch [5/10] Batch 700/938              Loss D: 1.6077, Loss G: 0.4664, D(x): 0.5389
Epoch [5/10] Batch 800/938              Loss D: 1.5404, Loss G: 0.5176, D(x): 0.5455
Epoch [5/10] Batch 900/938              Loss D: 1.6231, Loss G: 0.4719, D(x): 0.5216
Epoch [6/10] Batch 0/938              Loss D: 1.5849, Loss G: 0.4772, D(x): 0.5480
Epoch [6/10] Batch 100/938              Loss D: 1.5798, Loss G: 0.4829, D(x): 0.5439
Epoch [6/10] Batch 200/938              Loss D: 1.6223, Loss G: 0.4469, D(x): 0.5471
Epoch [6/10] Batch 300/938              Loss D: 1.5636, Loss G: 0.4990, D(x): 0.5379
Epoch [6/10] Batch 400/938              Loss D: 1.6149, Loss G: 0.4698, D(x): 0.5329
Epoch [6/10] Batch 500/938              Loss D: 1.5887, Loss G: 0.4751, D(x): 0.5386
Epoch [6/10] Batch 600/938              Loss D: 1.5585, Loss G: 0.4866, D(x): 0.5546
Epoch [6/10] Batch 700/938              Loss D: 1.6018, Loss G: 0.4749, D(x): 0.5349
Epoch [6/10] Batch 800/938              Loss D: 1.5891, Loss G: 0.4753, D(x): 0.5434
Epoch [6/10] Batch 900/938              Loss D: 1.5856, Loss G: 0.4791, D(x): 0.5348
Epoch [7/10] Batch 0/938              Loss D: 1.6441, Loss G: 0.4492, D(x): 0.5231
Epoch [7/10] Batch 100/938              Loss D: 1.5623, Loss G: 0.4890, D(x): 0.5483
Epoch [7/10] Batch 200/938              Loss D: 1.5675, Loss G: 0.4722, D(x): 0.5564
Epoch [7/10] Batch 300/938              Loss D: 1.6115, Loss G: 0.4758, D(x): 0.5243
Epoch [7/10] Batch 400/938              Loss D: 1.6219, Loss G: 0.4592, D(x): 0.5341
Epoch [7/10] Batch 500/938              Loss D: 1.5969, Loss G: 0.4779, D(x): 0.5308
Epoch [7/10] Batch 600/938              Loss D: 1.5901, Loss G: 0.4740, D(x): 0.5364
Epoch [7/10] Batch 700/938              Loss D: 1.5907, Loss G: 0.4863, D(x): 0.5296
Epoch [7/10] Batch 800/938              Loss D: 1.5591, Loss G: 0.4917, D(x): 0.5454
Epoch [7/10] Batch 900/938              Loss D: 1.5469, Loss G: 0.4975, D(x): 0.5466
Epoch [8/10] Batch 0/938              Loss D: 1.5598, Loss G: 0.4929, D(x): 0.5463
Epoch [8/10] Batch 100/938              Loss D: 1.6364, Loss G: 0.4572, D(x): 0.5214
Epoch [8/10] Batch 200/938              Loss D: 1.6077, Loss G: 0.4800, D(x): 0.5258
Epoch [8/10] Batch 300/938              Loss D: 1.6247, Loss G: 0.4726, D(x): 0.5205
Epoch [8/10] Batch 400/938              Loss D: 1.5896, Loss G: 0.4781, D(x): 0.5382
Epoch [8/10] Batch 500/938              Loss D: 1.5891, Loss G: 0.4876, D(x): 0.5305
Epoch [8/10] Batch 600/938              Loss D: 1.5711, Loss G: 0.4776, D(x): 0.5513
Epoch [8/10] Batch 700/938              Loss D: 1.5597, Loss G: 0.4763, D(x): 0.5604
Epoch [8/10] Batch 800/938              Loss D: 1.6197, Loss G: 0.4671, D(x): 0.5317
Epoch [8/10] Batch 900/938              Loss D: 1.6104, Loss G: 0.4705, D(x): 0.5323
Epoch [9/10] Batch 0/938              Loss D: 1.5791, Loss G: 0.4829, D(x): 0.5393
Epoch [9/10] Batch 100/938              Loss D: 1.6038, Loss G: 0.4659, D(x): 0.5365
Epoch [9/10] Batch 200/938              Loss D: 1.5625, Loss G: 0.5044, D(x): 0.5394
Epoch [9/10] Batch 300/938              Loss D: 1.5941, Loss G: 0.4681, D(x): 0.5482
Epoch [9/10] Batch 400/938              Loss D: 1.5999, Loss G: 0.4724, D(x): 0.5326
Epoch [9/10] Batch 500/938              Loss D: 1.5792, Loss G: 0.4889, D(x): 0.5351
Epoch [9/10] Batch 600/938              Loss D: 1.5984, Loss G: 0.4982, D(x): 0.5154
Epoch [9/10] Batch 700/938              Loss D: 1.5943, Loss G: 0.4913, D(x): 0.5297
Epoch [9/10] Batch 800/938              Loss D: 1.5953, Loss G: 0.4879, D(x): 0.5256
Epoch [9/10] Batch 900/938              Loss D: 1.5709, Loss G: 0.4807, D(x): 0.5456



Observations:

FID score: 0.02773356974363539
Inception score: 1.1718

Training Time: 15min 24sec

Loss D: 1.5709
Loss G: 0.4807
D(x): 0.5456

Notes:
Looking at the FID score and IS score, it's more or less close to what the base
results were (though there is actually some improvement in the loss).
However, one look at the resulting images (which is basically static, with no clear numbers
like the base dcgan) to tell us that the model has undergone something like
a 'mode collapse'. Additionally the training time increased by about 3 minutes

The main changes were made to the Generator model, mostly through reducing the 5 Conv2dTranpose
layers to 3 conv2dTranpose, which resulted in some drastic changes to the kernel size and stride
to maintain the n x channels_img x 64 x 64 shape of the base. These changes resulted in a
drastically lower performance
