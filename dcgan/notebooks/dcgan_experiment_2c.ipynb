{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:24.646874Z",
     "start_time": "2024-10-21T21:38:21.194358Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:24.654572Z",
     "start_time": "2024-10-21T21:38:24.650382Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.__version__)",
   "id": "9dc942bfe103e1c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:25.214636Z",
     "start_time": "2024-10-21T21:38:25.179215Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.is_available())",
   "id": "137b405b8ec86171",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:25.237969Z",
     "start_time": "2024-10-21T21:38:25.233161Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.version.cuda)",
   "id": "d152c5800b640b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:25.264637Z",
     "start_time": "2024-10-21T21:38:25.261128Z"
    }
   },
   "cell_type": "code",
   "source": "tensor_cpu = torch.randn(3, 3)",
   "id": "5629e1d8ea31d4b3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:25.421861Z",
     "start_time": "2024-10-21T21:38:25.319943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "tensor_gpu = tensor_cpu.to('cuda')"
   ],
   "id": "c2e24cd9d00a17f5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:25.559587Z",
     "start_time": "2024-10-21T21:38:25.555020Z"
    }
   },
   "cell_type": "code",
   "source": "print(tensor_gpu.device)",
   "id": "54aa193243890f6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.211790Z",
     "start_time": "2024-10-21T21:38:25.573441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as dataloader\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ],
   "id": "d96150f2a15503f9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.232993Z",
     "start_time": "2024-10-21T21:38:42.224567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # is the convention for Conv in pytorch N, channels, height, width?\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), # what does padding 1 correspond to?\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1), # why features_d for filters? Why features_d * 2?\n",
    "            nn.BatchNorm2d(features_d * 2), # because GANS are known for being notoriously unstable during training -- why are GANS known for this?\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features_d *2, features_d * 4, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(features_d * 4), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d * 4, features_d * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_d * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # N x features_d * 8 x 4 x 4\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            \n",
    "            # N x 1 x 1 x 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n"
   ],
   "id": "f86d46c295864940",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.251934Z",
     "start_time": "2024-10-21T21:38:42.244438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # N x channels_noise x 1 x 1\n",
    "            nn.ConvTranspose2d(channels_noise, features_g * 16, kernel_size=4, stride=1, padding=0), \n",
    "            nn.BatchNorm2d(features_g * 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # N x features_g * 16 x 4 x 4\n",
    "            nn.ConvTranspose2d(features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g * 8, features_g* 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            \n",
    "            # N x channels_img # 64 x 64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "8c8407fe0a2f6c65",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.268724Z",
     "start_time": "2024-10-21T21:38:42.264361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "image_size = 64 # 28 x 28 >>> 64x64\n",
    "channels_img = 1\n",
    "channels_noise = 256\n",
    "\n",
    "features_d = 16 # was set at 64 in the paper but not needed for mnist might for celebrity faces though\n",
    "features_g = 16 # was set at 64 in the paper but not needed for mnist might for celebrity faces though"
   ],
   "id": "5fbde815fb0b2678",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.287237Z",
     "start_time": "2024-10-21T21:38:42.281363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_transforms = transforms.Compose([ # what does the transforms do in pytorch??\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ],
   "id": "815cd660a175607c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.354982Z",
     "start_time": "2024-10-21T21:38:42.300403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = datasets.MNIST(root='dataset/', train=True, transform=my_transforms, download=True) # seems to download the specified dataset to my directory\n",
    "my_dataloader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) # not sure what's going on thought I already imported this?\n"
   ],
   "id": "d0f0f89ecd3793b7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.372791Z",
     "start_time": "2024-10-21T21:38:42.367899Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
   "id": "dbc35f8de6c91776",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.458777Z",
     "start_time": "2024-10-21T21:38:42.389873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create discriminator and generator\n",
    "\n",
    "netD = Discriminator(channels_img, features_d).to(device) #  Iguess every model has to be specified to a device? So I could run some on different gpus or my cpu? \n",
    "netG = Generator(channels_noise, channels_img, features_g).to(device)"
   ],
   "id": "305e2080cd6e3b24",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.481537Z",
     "start_time": "2024-10-21T21:38:42.475441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup Optimizer for G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999)) # why are we specifying? What is the default betas value? 0.9 and 0.999?\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
   ],
   "id": "396764a0e16d0f28",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.513861Z",
     "start_time": "2024-10-21T21:38:42.502602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "netD.train() # what is the difference being in training mode from otherwise in pytorch?"
   ],
   "id": "1e043c6a90181cff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.614699Z",
     "start_time": "2024-10-21T21:38:42.607412Z"
    }
   },
   "cell_type": "code",
   "source": "netG.train() # apparently the models should be in training mode by default but we're doing it explicitly",
   "id": "f6f362d3c428d92e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.734714Z",
     "start_time": "2024-10-21T21:38:42.730621Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.BCELoss() # binary cross entropy -- should probably ask why this specific loss function?",
   "id": "49dae3fcf107a6c3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.772316Z",
     "start_time": "2024-10-21T21:38:42.768282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ],
   "id": "c42132f3e272208b",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.818673Z",
     "start_time": "2024-10-21T21:38:42.813394Z"
    }
   },
   "cell_type": "code",
   "source": "fixed_noise = torch.randn(64, channels_noise, 1, 1).to(device)",
   "id": "1cf6a8e05dcfa525",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:38:42.829365Z",
     "start_time": "2024-10-21T21:38:42.825102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.utils as vutils"
   ],
   "id": "159eef8860ee7471",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:06.516036Z",
     "start_time": "2024-10-21T21:38:42.861247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"starting training...\")\n",
    "num_epochs = 50\n",
    "img_idx = 0\n",
    "\n",
    "# writer_real = SummaryWriter(log_dir='runs/GAN_MNIST/log_real')\n",
    "# writer_fake = SummaryWriter(log_dir='runs/GAN_MNIST/log_fake')\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(my_dataloader):\n",
    "        data = data.to(device)\n",
    "        batch_size = data.shape[0]\n",
    "        # apparently it's important that we train the Discriminator first\n",
    "        # Train Discriminator : max log (D(x)) + log(1-D(G(z)))\n",
    "        # We send in all real images first\n",
    "        netD.zero_grad()\n",
    "        label = (torch.ones(batch_size)*0.9).to(device) # apparently 0.9 helps? Would like to look at that again\n",
    "        output = netD(data).reshape(-1)\n",
    "        \n",
    "        lossD_real = criterion(output, label)\n",
    "        D_x = output.mean().item() # for evaluation purposes, could do something similar with the other models to get \n",
    "        \n",
    "        # now we send in all fake images to the discriminator\n",
    "        noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)\n",
    "        fake = netG(noise)\n",
    "        label = (torch.ones(batch_size)*0.1).to(device) # would what to look at this again\n",
    "        \n",
    "        output = netD(fake.detach()).reshape(-1) # telling pytorch not to trace the gradients? Not exactly sure, does that mean we're not training the generator? \n",
    "        lossD_fake = criterion(output, label)\n",
    "        \n",
    "        lossD = lossD_real + lossD_fake\n",
    "        lossD.backward() # not sure what these are doing -- look this up\n",
    "        optimizerD.step() # need to ask about this\n",
    "        \n",
    "        # Train Generator: min log(1 - D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label = torch.ones(batch_size).to(device) # not multiplying by 0.9 here\n",
    "        output = netD(fake).reshape(-1) # we actually want to train the generator now? So we don't do detach?\n",
    "        lossG = criterion(output, label)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(my_dataloader)} \\\n",
    "             Loss D: {lossD:.4f}, Loss G: {lossG:.4f}, D(x): {D_x:.4f}\")\n",
    "            \n",
    "            with torch.no_grad(): # what is happening here?\n",
    "                fake = netG(fixed_noise)\n",
    "                \n",
    "                img_grid_real = torchvision.utils.make_grid(data[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                # writer_real.add_image('MNIST Real Images', img_grid_real)\n",
    "                # writer_fake.add_image('MNIST Fake Images', img_grid_fake)\n",
    "                \n",
    "                #since Tensorboard site is still being allowed, I'll provide a locall method of viewing the pictures\n",
    "                to_pil = ToPILImage()\n",
    "                \n",
    "                # img_real = to_pil(img_grid_real)\n",
    "                # img_real.save(f'../images/experiment2/b/real/real_images_grid_{img_idx}.png')\n",
    "                \n",
    "                img_fake = to_pil(img_grid_fake)\n",
    "                img_fake.save(f'../images/experiment2/fake/c/fake_images_grid_{img_idx}.png')\n",
    "                img_idx += 1\n",
    "        "
   ],
   "id": "4e8594e0da17b164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch [0/50] Batch 0/938              Loss D: 1.2699, Loss G: 0.9837, D(x): 0.5941\n",
      "Epoch [0/50] Batch 100/938              Loss D: 0.6907, Loss G: 3.7084, D(x): 0.8761\n",
      "Epoch [0/50] Batch 200/938              Loss D: 0.6689, Loss G: 3.0151, D(x): 0.8928\n",
      "Epoch [0/50] Batch 300/938              Loss D: 0.7162, Loss G: 2.2155, D(x): 0.8244\n",
      "Epoch [0/50] Batch 400/938              Loss D: 0.7185, Loss G: 2.7692, D(x): 0.9333\n",
      "Epoch [0/50] Batch 500/938              Loss D: 0.7202, Loss G: 2.6382, D(x): 0.8880\n",
      "Epoch [0/50] Batch 600/938              Loss D: 0.7331, Loss G: 2.2013, D(x): 0.8094\n",
      "Epoch [0/50] Batch 700/938              Loss D: 1.1902, Loss G: 4.9518, D(x): 0.9389\n",
      "Epoch [0/50] Batch 800/938              Loss D: 0.7607, Loss G: 1.6347, D(x): 0.7367\n",
      "Epoch [0/50] Batch 900/938              Loss D: 0.8507, Loss G: 0.9849, D(x): 0.6666\n",
      "Epoch [1/50] Batch 0/938              Loss D: 0.9161, Loss G: 2.8618, D(x): 0.8890\n",
      "Epoch [1/50] Batch 100/938              Loss D: 1.4282, Loss G: 2.9283, D(x): 0.9636\n",
      "Epoch [1/50] Batch 200/938              Loss D: 0.7935, Loss G: 2.2469, D(x): 0.9142\n",
      "Epoch [1/50] Batch 300/938              Loss D: 0.8052, Loss G: 1.0694, D(x): 0.6900\n",
      "Epoch [1/50] Batch 400/938              Loss D: 0.7576, Loss G: 1.5491, D(x): 0.7928\n",
      "Epoch [1/50] Batch 500/938              Loss D: 0.7425, Loss G: 1.6133, D(x): 0.7938\n",
      "Epoch [1/50] Batch 600/938              Loss D: 0.7314, Loss G: 2.3066, D(x): 0.8388\n",
      "Epoch [1/50] Batch 700/938              Loss D: 0.7186, Loss G: 2.0807, D(x): 0.8585\n",
      "Epoch [1/50] Batch 800/938              Loss D: 0.8279, Loss G: 1.4957, D(x): 0.6770\n",
      "Epoch [1/50] Batch 900/938              Loss D: 0.8417, Loss G: 1.2801, D(x): 0.6595\n",
      "Epoch [2/50] Batch 0/938              Loss D: 0.7063, Loss G: 1.6967, D(x): 0.8193\n",
      "Epoch [2/50] Batch 100/938              Loss D: 0.7154, Loss G: 2.0368, D(x): 0.8182\n",
      "Epoch [2/50] Batch 200/938              Loss D: 0.8523, Loss G: 2.6026, D(x): 0.9062\n",
      "Epoch [2/50] Batch 300/938              Loss D: 0.8008, Loss G: 1.3777, D(x): 0.6900\n",
      "Epoch [2/50] Batch 400/938              Loss D: 0.7727, Loss G: 2.7451, D(x): 0.8848\n",
      "Epoch [2/50] Batch 500/938              Loss D: 0.7827, Loss G: 1.4786, D(x): 0.7382\n",
      "Epoch [2/50] Batch 600/938              Loss D: 0.9271, Loss G: 1.5134, D(x): 0.8046\n",
      "Epoch [2/50] Batch 700/938              Loss D: 0.7187, Loss G: 1.6560, D(x): 0.8359\n",
      "Epoch [2/50] Batch 800/938              Loss D: 0.7017, Loss G: 2.0921, D(x): 0.8147\n",
      "Epoch [2/50] Batch 900/938              Loss D: 0.7648, Loss G: 1.6458, D(x): 0.7572\n",
      "Epoch [3/50] Batch 0/938              Loss D: 0.7406, Loss G: 1.9663, D(x): 0.7926\n",
      "Epoch [3/50] Batch 100/938              Loss D: 0.7761, Loss G: 1.9162, D(x): 0.8504\n",
      "Epoch [3/50] Batch 200/938              Loss D: 1.2288, Loss G: 0.6972, D(x): 0.4256\n",
      "Epoch [3/50] Batch 300/938              Loss D: 0.7673, Loss G: 2.4114, D(x): 0.8698\n",
      "Epoch [3/50] Batch 400/938              Loss D: 1.9910, Loss G: 0.6287, D(x): 0.1904\n",
      "Epoch [3/50] Batch 500/938              Loss D: 0.7608, Loss G: 1.7602, D(x): 0.7858\n",
      "Epoch [3/50] Batch 600/938              Loss D: 0.7450, Loss G: 1.7324, D(x): 0.7774\n",
      "Epoch [3/50] Batch 700/938              Loss D: 1.0366, Loss G: 2.4419, D(x): 0.9494\n",
      "Epoch [3/50] Batch 800/938              Loss D: 0.7437, Loss G: 1.8054, D(x): 0.7922\n",
      "Epoch [3/50] Batch 900/938              Loss D: 0.7190, Loss G: 2.2121, D(x): 0.8416\n",
      "Epoch [4/50] Batch 0/938              Loss D: 0.7547, Loss G: 2.1170, D(x): 0.8637\n",
      "Epoch [4/50] Batch 100/938              Loss D: 0.8120, Loss G: 2.4374, D(x): 0.8611\n",
      "Epoch [4/50] Batch 200/938              Loss D: 0.8070, Loss G: 1.6496, D(x): 0.7122\n",
      "Epoch [4/50] Batch 300/938              Loss D: 0.7762, Loss G: 1.4364, D(x): 0.7608\n",
      "Epoch [4/50] Batch 400/938              Loss D: 0.7363, Loss G: 2.2189, D(x): 0.8368\n",
      "Epoch [4/50] Batch 500/938              Loss D: 0.7527, Loss G: 1.5093, D(x): 0.7610\n",
      "Epoch [4/50] Batch 600/938              Loss D: 0.7708, Loss G: 1.6767, D(x): 0.8587\n",
      "Epoch [4/50] Batch 700/938              Loss D: 0.7536, Loss G: 1.7588, D(x): 0.7646\n",
      "Epoch [4/50] Batch 800/938              Loss D: 0.7925, Loss G: 2.5969, D(x): 0.8620\n",
      "Epoch [4/50] Batch 900/938              Loss D: 0.9762, Loss G: 3.7152, D(x): 0.9562\n",
      "Epoch [5/50] Batch 0/938              Loss D: 0.7273, Loss G: 1.7003, D(x): 0.8107\n",
      "Epoch [5/50] Batch 100/938              Loss D: 0.7688, Loss G: 3.2126, D(x): 0.8942\n",
      "Epoch [5/50] Batch 200/938              Loss D: 0.7505, Loss G: 1.6855, D(x): 0.8372\n",
      "Epoch [5/50] Batch 300/938              Loss D: 0.7438, Loss G: 1.9554, D(x): 0.8428\n",
      "Epoch [5/50] Batch 400/938              Loss D: 0.6890, Loss G: 1.9996, D(x): 0.8532\n",
      "Epoch [5/50] Batch 500/938              Loss D: 0.7695, Loss G: 2.0857, D(x): 0.8854\n",
      "Epoch [5/50] Batch 600/938              Loss D: 0.7969, Loss G: 1.7527, D(x): 0.8012\n",
      "Epoch [5/50] Batch 700/938              Loss D: 0.7340, Loss G: 2.3543, D(x): 0.8283\n",
      "Epoch [5/50] Batch 800/938              Loss D: 0.7123, Loss G: 1.7319, D(x): 0.7995\n",
      "Epoch [5/50] Batch 900/938              Loss D: 0.8009, Loss G: 1.3762, D(x): 0.7028\n",
      "Epoch [6/50] Batch 0/938              Loss D: 0.8434, Loss G: 2.3014, D(x): 0.8992\n",
      "Epoch [6/50] Batch 100/938              Loss D: 0.7485, Loss G: 1.9236, D(x): 0.7606\n",
      "Epoch [6/50] Batch 200/938              Loss D: 1.1266, Loss G: 0.9353, D(x): 0.4746\n",
      "Epoch [6/50] Batch 300/938              Loss D: 0.8501, Loss G: 2.6439, D(x): 0.8961\n",
      "Epoch [6/50] Batch 400/938              Loss D: 0.7808, Loss G: 1.4400, D(x): 0.7183\n",
      "Epoch [6/50] Batch 500/938              Loss D: 0.7093, Loss G: 1.8553, D(x): 0.8338\n",
      "Epoch [6/50] Batch 600/938              Loss D: 0.7462, Loss G: 1.7648, D(x): 0.7661\n",
      "Epoch [6/50] Batch 700/938              Loss D: 0.7155, Loss G: 2.0204, D(x): 0.8178\n",
      "Epoch [6/50] Batch 800/938              Loss D: 0.7252, Loss G: 1.8442, D(x): 0.8038\n",
      "Epoch [6/50] Batch 900/938              Loss D: 0.6961, Loss G: 2.1823, D(x): 0.8347\n",
      "Epoch [7/50] Batch 0/938              Loss D: 0.6982, Loss G: 2.3540, D(x): 0.8915\n",
      "Epoch [7/50] Batch 100/938              Loss D: 0.7046, Loss G: 2.1779, D(x): 0.8858\n",
      "Epoch [7/50] Batch 200/938              Loss D: 0.7130, Loss G: 1.8492, D(x): 0.8026\n",
      "Epoch [7/50] Batch 300/938              Loss D: 0.7848, Loss G: 1.5682, D(x): 0.7117\n",
      "Epoch [7/50] Batch 400/938              Loss D: 0.7683, Loss G: 1.8410, D(x): 0.8076\n",
      "Epoch [7/50] Batch 500/938              Loss D: 1.1205, Loss G: 2.1285, D(x): 0.5845\n",
      "Epoch [7/50] Batch 600/938              Loss D: 0.7125, Loss G: 1.7150, D(x): 0.7941\n",
      "Epoch [7/50] Batch 700/938              Loss D: 0.7067, Loss G: 1.5046, D(x): 0.8232\n",
      "Epoch [7/50] Batch 800/938              Loss D: 0.6800, Loss G: 2.1609, D(x): 0.8587\n",
      "Epoch [7/50] Batch 900/938              Loss D: 0.7052, Loss G: 2.1284, D(x): 0.8933\n",
      "Epoch [8/50] Batch 0/938              Loss D: 1.3002, Loss G: 0.9601, D(x): 0.4137\n",
      "Epoch [8/50] Batch 100/938              Loss D: 0.7362, Loss G: 1.7265, D(x): 0.7862\n",
      "Epoch [8/50] Batch 200/938              Loss D: 0.7236, Loss G: 2.4963, D(x): 0.8980\n",
      "Epoch [8/50] Batch 300/938              Loss D: 0.8996, Loss G: 3.4590, D(x): 0.9584\n",
      "Epoch [8/50] Batch 400/938              Loss D: 0.7402, Loss G: 1.6441, D(x): 0.7625\n",
      "Epoch [8/50] Batch 500/938              Loss D: 0.7081, Loss G: 2.0769, D(x): 0.8194\n",
      "Epoch [8/50] Batch 600/938              Loss D: 0.7118, Loss G: 2.8464, D(x): 0.9205\n",
      "Epoch [8/50] Batch 700/938              Loss D: 0.6845, Loss G: 2.2643, D(x): 0.8435\n",
      "Epoch [8/50] Batch 800/938              Loss D: 0.6829, Loss G: 1.8465, D(x): 0.8465\n",
      "Epoch [8/50] Batch 900/938              Loss D: 0.8109, Loss G: 1.5746, D(x): 0.6924\n",
      "Epoch [9/50] Batch 0/938              Loss D: 0.6756, Loss G: 2.2552, D(x): 0.8734\n",
      "Epoch [9/50] Batch 100/938              Loss D: 0.6991, Loss G: 2.7553, D(x): 0.9217\n",
      "Epoch [9/50] Batch 200/938              Loss D: 0.8449, Loss G: 1.4194, D(x): 0.7355\n",
      "Epoch [9/50] Batch 300/938              Loss D: 0.7232, Loss G: 1.7261, D(x): 0.7948\n",
      "Epoch [9/50] Batch 400/938              Loss D: 0.7003, Loss G: 1.9478, D(x): 0.8597\n",
      "Epoch [9/50] Batch 500/938              Loss D: 0.6802, Loss G: 2.5408, D(x): 0.8773\n",
      "Epoch [9/50] Batch 600/938              Loss D: 0.7858, Loss G: 2.9419, D(x): 0.9091\n",
      "Epoch [9/50] Batch 700/938              Loss D: 0.6924, Loss G: 2.0891, D(x): 0.9128\n",
      "Epoch [9/50] Batch 800/938              Loss D: 0.7850, Loss G: 2.7997, D(x): 0.9357\n",
      "Epoch [9/50] Batch 900/938              Loss D: 0.7672, Loss G: 3.3973, D(x): 0.9450\n",
      "Epoch [10/50] Batch 0/938              Loss D: 0.8110, Loss G: 0.9836, D(x): 0.6983\n",
      "Epoch [10/50] Batch 100/938              Loss D: 0.8486, Loss G: 2.0416, D(x): 0.8362\n",
      "Epoch [10/50] Batch 200/938              Loss D: 0.8187, Loss G: 3.1023, D(x): 0.9475\n",
      "Epoch [10/50] Batch 300/938              Loss D: 1.2878, Loss G: 0.9186, D(x): 0.3992\n",
      "Epoch [10/50] Batch 400/938              Loss D: 0.6872, Loss G: 1.9570, D(x): 0.8564\n",
      "Epoch [10/50] Batch 500/938              Loss D: 0.6985, Loss G: 2.6632, D(x): 0.9365\n",
      "Epoch [10/50] Batch 600/938              Loss D: 0.7185, Loss G: 2.4872, D(x): 0.8984\n",
      "Epoch [10/50] Batch 700/938              Loss D: 0.6895, Loss G: 2.0047, D(x): 0.8716\n",
      "Epoch [10/50] Batch 800/938              Loss D: 0.6816, Loss G: 2.2320, D(x): 0.8661\n",
      "Epoch [10/50] Batch 900/938              Loss D: 0.7232, Loss G: 2.0833, D(x): 0.7850\n",
      "Epoch [11/50] Batch 0/938              Loss D: 0.6867, Loss G: 2.8627, D(x): 0.9293\n",
      "Epoch [11/50] Batch 100/938              Loss D: 0.8178, Loss G: 1.6323, D(x): 0.7577\n",
      "Epoch [11/50] Batch 200/938              Loss D: 0.7113, Loss G: 1.7810, D(x): 0.7991\n",
      "Epoch [11/50] Batch 300/938              Loss D: 0.6885, Loss G: 2.1632, D(x): 0.8310\n",
      "Epoch [11/50] Batch 400/938              Loss D: 0.6931, Loss G: 2.7192, D(x): 0.9176\n",
      "Epoch [11/50] Batch 500/938              Loss D: 0.6875, Loss G: 2.2012, D(x): 0.8330\n",
      "Epoch [11/50] Batch 600/938              Loss D: 0.6730, Loss G: 2.6042, D(x): 0.9063\n",
      "Epoch [11/50] Batch 700/938              Loss D: 0.6863, Loss G: 2.3952, D(x): 0.8418\n",
      "Epoch [11/50] Batch 800/938              Loss D: 0.7175, Loss G: 2.9428, D(x): 0.9081\n",
      "Epoch [11/50] Batch 900/938              Loss D: 0.6726, Loss G: 2.0252, D(x): 0.8702\n",
      "Epoch [12/50] Batch 0/938              Loss D: 0.6709, Loss G: 2.5307, D(x): 0.9138\n",
      "Epoch [12/50] Batch 100/938              Loss D: 0.7256, Loss G: 1.9062, D(x): 0.8714\n",
      "Epoch [12/50] Batch 200/938              Loss D: 0.6760, Loss G: 2.3154, D(x): 0.8662\n",
      "Epoch [12/50] Batch 300/938              Loss D: 0.6709, Loss G: 2.0818, D(x): 0.8952\n",
      "Epoch [12/50] Batch 400/938              Loss D: 0.6669, Loss G: 2.4390, D(x): 0.8992\n",
      "Epoch [12/50] Batch 500/938              Loss D: 0.7112, Loss G: 2.8275, D(x): 0.9096\n",
      "Epoch [12/50] Batch 600/938              Loss D: 0.7848, Loss G: 2.9020, D(x): 0.9465\n",
      "Epoch [12/50] Batch 700/938              Loss D: 0.6685, Loss G: 2.4633, D(x): 0.9033\n",
      "Epoch [12/50] Batch 800/938              Loss D: 0.6877, Loss G: 2.6735, D(x): 0.8975\n",
      "Epoch [12/50] Batch 900/938              Loss D: 0.7710, Loss G: 2.8778, D(x): 0.9496\n",
      "Epoch [13/50] Batch 0/938              Loss D: 0.6724, Loss G: 2.5216, D(x): 0.9015\n",
      "Epoch [13/50] Batch 100/938              Loss D: 0.7355, Loss G: 2.8396, D(x): 0.9339\n",
      "Epoch [13/50] Batch 200/938              Loss D: 0.7653, Loss G: 2.8282, D(x): 0.9172\n",
      "Epoch [13/50] Batch 300/938              Loss D: 0.7305, Loss G: 2.8945, D(x): 0.9554\n",
      "Epoch [13/50] Batch 400/938              Loss D: 0.6691, Loss G: 2.4812, D(x): 0.8894\n",
      "Epoch [13/50] Batch 500/938              Loss D: 0.6937, Loss G: 2.5549, D(x): 0.9039\n",
      "Epoch [13/50] Batch 600/938              Loss D: 0.7262, Loss G: 1.6604, D(x): 0.7707\n",
      "Epoch [13/50] Batch 700/938              Loss D: 0.6757, Loss G: 2.0309, D(x): 0.8501\n",
      "Epoch [13/50] Batch 800/938              Loss D: 0.7304, Loss G: 3.4207, D(x): 0.9212\n",
      "Epoch [13/50] Batch 900/938              Loss D: 0.7247, Loss G: 2.8444, D(x): 0.9578\n",
      "Epoch [14/50] Batch 0/938              Loss D: 0.6750, Loss G: 2.5001, D(x): 0.9354\n",
      "Epoch [14/50] Batch 100/938              Loss D: 0.7470, Loss G: 3.1959, D(x): 0.9475\n",
      "Epoch [14/50] Batch 200/938              Loss D: 0.6813, Loss G: 2.3086, D(x): 0.8860\n",
      "Epoch [14/50] Batch 300/938              Loss D: 0.6672, Loss G: 2.1923, D(x): 0.8984\n",
      "Epoch [14/50] Batch 400/938              Loss D: 0.6833, Loss G: 2.3711, D(x): 0.8799\n",
      "Epoch [14/50] Batch 500/938              Loss D: 0.6811, Loss G: 2.3877, D(x): 0.9398\n",
      "Epoch [14/50] Batch 600/938              Loss D: 0.7027, Loss G: 2.5354, D(x): 0.9167\n",
      "Epoch [14/50] Batch 700/938              Loss D: 0.6703, Loss G: 2.3389, D(x): 0.8803\n",
      "Epoch [14/50] Batch 800/938              Loss D: 0.7103, Loss G: 2.0980, D(x): 0.8173\n",
      "Epoch [14/50] Batch 900/938              Loss D: 0.7860, Loss G: 1.6071, D(x): 0.7172\n",
      "Epoch [15/50] Batch 0/938              Loss D: 0.6674, Loss G: 2.3031, D(x): 0.8880\n",
      "Epoch [15/50] Batch 100/938              Loss D: 0.7878, Loss G: 1.6512, D(x): 0.7111\n",
      "Epoch [15/50] Batch 200/938              Loss D: 0.7321, Loss G: 3.0744, D(x): 0.9470\n",
      "Epoch [15/50] Batch 300/938              Loss D: 0.6709, Loss G: 2.2350, D(x): 0.8995\n",
      "Epoch [15/50] Batch 400/938              Loss D: 0.6686, Loss G: 2.2814, D(x): 0.8884\n",
      "Epoch [15/50] Batch 500/938              Loss D: 0.6668, Loss G: 2.3854, D(x): 0.9053\n",
      "Epoch [15/50] Batch 600/938              Loss D: 0.6818, Loss G: 2.1222, D(x): 0.8786\n",
      "Epoch [15/50] Batch 700/938              Loss D: 0.6646, Loss G: 2.4324, D(x): 0.8972\n",
      "Epoch [15/50] Batch 800/938              Loss D: 0.7059, Loss G: 1.8071, D(x): 0.7929\n",
      "Epoch [15/50] Batch 900/938              Loss D: 0.6838, Loss G: 1.8475, D(x): 0.8529\n",
      "Epoch [16/50] Batch 0/938              Loss D: 0.7057, Loss G: 1.7750, D(x): 0.8078\n",
      "Epoch [16/50] Batch 100/938              Loss D: 0.7915, Loss G: 1.5563, D(x): 0.7384\n",
      "Epoch [16/50] Batch 200/938              Loss D: 0.7048, Loss G: 2.4808, D(x): 0.9068\n",
      "Epoch [16/50] Batch 300/938              Loss D: 0.6689, Loss G: 2.3530, D(x): 0.8832\n",
      "Epoch [16/50] Batch 400/938              Loss D: 0.7190, Loss G: 2.8066, D(x): 0.9199\n",
      "Epoch [16/50] Batch 500/938              Loss D: 0.6704, Loss G: 2.2184, D(x): 0.8692\n",
      "Epoch [16/50] Batch 600/938              Loss D: 0.6704, Loss G: 2.3125, D(x): 0.8825\n",
      "Epoch [16/50] Batch 700/938              Loss D: 0.6659, Loss G: 2.3088, D(x): 0.8857\n",
      "Epoch [16/50] Batch 800/938              Loss D: 0.6718, Loss G: 2.4393, D(x): 0.9077\n",
      "Epoch [16/50] Batch 900/938              Loss D: 0.6728, Loss G: 2.2833, D(x): 0.8634\n",
      "Epoch [17/50] Batch 0/938              Loss D: 0.6668, Loss G: 2.3816, D(x): 0.8922\n",
      "Epoch [17/50] Batch 100/938              Loss D: 0.6709, Loss G: 2.2413, D(x): 0.8902\n",
      "Epoch [17/50] Batch 200/938              Loss D: 0.7337, Loss G: 3.0064, D(x): 0.9365\n",
      "Epoch [17/50] Batch 300/938              Loss D: 0.6688, Loss G: 2.3322, D(x): 0.8746\n",
      "Epoch [17/50] Batch 400/938              Loss D: 0.6874, Loss G: 2.2037, D(x): 0.8353\n",
      "Epoch [17/50] Batch 500/938              Loss D: 0.6831, Loss G: 1.8775, D(x): 0.8295\n",
      "Epoch [17/50] Batch 600/938              Loss D: 0.6640, Loss G: 2.2612, D(x): 0.8945\n",
      "Epoch [17/50] Batch 700/938              Loss D: 0.9446, Loss G: 0.9090, D(x): 0.5679\n",
      "Epoch [17/50] Batch 800/938              Loss D: 0.6718, Loss G: 2.1233, D(x): 0.8924\n",
      "Epoch [17/50] Batch 900/938              Loss D: 0.6737, Loss G: 2.3903, D(x): 0.8606\n",
      "Epoch [18/50] Batch 0/938              Loss D: 0.6731, Loss G: 2.3850, D(x): 0.9091\n",
      "Epoch [18/50] Batch 100/938              Loss D: 0.6677, Loss G: 2.1284, D(x): 0.8724\n",
      "Epoch [18/50] Batch 200/938              Loss D: 0.8738, Loss G: 1.6073, D(x): 0.6367\n",
      "Epoch [18/50] Batch 300/938              Loss D: 0.6816, Loss G: 2.2439, D(x): 0.8385\n",
      "Epoch [18/50] Batch 400/938              Loss D: 0.6677, Loss G: 2.3609, D(x): 0.8887\n",
      "Epoch [18/50] Batch 500/938              Loss D: 0.6789, Loss G: 1.8585, D(x): 0.8327\n",
      "Epoch [18/50] Batch 600/938              Loss D: 0.6696, Loss G: 2.4747, D(x): 0.9129\n",
      "Epoch [18/50] Batch 700/938              Loss D: 0.6671, Loss G: 2.4952, D(x): 0.8749\n",
      "Epoch [18/50] Batch 800/938              Loss D: 0.6592, Loss G: 2.2985, D(x): 0.9068\n",
      "Epoch [18/50] Batch 900/938              Loss D: 0.6613, Loss G: 2.4538, D(x): 0.9099\n",
      "Epoch [19/50] Batch 0/938              Loss D: 0.6608, Loss G: 2.1668, D(x): 0.9059\n",
      "Epoch [19/50] Batch 100/938              Loss D: 0.6699, Loss G: 2.5116, D(x): 0.9180\n",
      "Epoch [19/50] Batch 200/938              Loss D: 0.6680, Loss G: 2.2379, D(x): 0.8727\n",
      "Epoch [19/50] Batch 300/938              Loss D: 0.6610, Loss G: 2.5400, D(x): 0.9153\n",
      "Epoch [19/50] Batch 400/938              Loss D: 0.6785, Loss G: 1.9734, D(x): 0.8409\n",
      "Epoch [19/50] Batch 500/938              Loss D: 0.6744, Loss G: 1.9605, D(x): 0.8463\n",
      "Epoch [19/50] Batch 600/938              Loss D: 0.6614, Loss G: 2.4509, D(x): 0.9105\n",
      "Epoch [19/50] Batch 700/938              Loss D: 0.6713, Loss G: 2.3309, D(x): 0.8574\n",
      "Epoch [19/50] Batch 800/938              Loss D: 1.1867, Loss G: 1.1124, D(x): 0.4559\n",
      "Epoch [19/50] Batch 900/938              Loss D: 0.6696, Loss G: 2.0992, D(x): 0.9222\n",
      "Epoch [20/50] Batch 0/938              Loss D: 0.6765, Loss G: 2.6615, D(x): 0.9387\n",
      "Epoch [20/50] Batch 100/938              Loss D: 0.6625, Loss G: 2.3065, D(x): 0.8858\n",
      "Epoch [20/50] Batch 200/938              Loss D: 0.6620, Loss G: 2.3347, D(x): 0.8858\n",
      "Epoch [20/50] Batch 300/938              Loss D: 0.6762, Loss G: 2.5996, D(x): 0.9329\n",
      "Epoch [20/50] Batch 400/938              Loss D: 0.6650, Loss G: 2.2598, D(x): 0.9128\n",
      "Epoch [20/50] Batch 500/938              Loss D: 0.6608, Loss G: 2.4606, D(x): 0.8993\n",
      "Epoch [20/50] Batch 600/938              Loss D: 0.6625, Loss G: 2.3204, D(x): 0.9018\n",
      "Epoch [20/50] Batch 700/938              Loss D: 0.6609, Loss G: 2.2927, D(x): 0.9150\n",
      "Epoch [20/50] Batch 800/938              Loss D: 1.2362, Loss G: 1.6789, D(x): 0.6824\n",
      "Epoch [20/50] Batch 900/938              Loss D: 0.6819, Loss G: 2.0653, D(x): 0.8404\n",
      "Epoch [21/50] Batch 0/938              Loss D: 0.7030, Loss G: 1.9674, D(x): 0.8039\n",
      "Epoch [21/50] Batch 100/938              Loss D: 0.6904, Loss G: 2.0943, D(x): 0.8258\n",
      "Epoch [21/50] Batch 200/938              Loss D: 0.6835, Loss G: 2.5728, D(x): 0.9340\n",
      "Epoch [21/50] Batch 300/938              Loss D: 0.6651, Loss G: 2.1960, D(x): 0.8874\n",
      "Epoch [21/50] Batch 400/938              Loss D: 0.6623, Loss G: 2.0424, D(x): 0.8763\n",
      "Epoch [21/50] Batch 500/938              Loss D: 0.6582, Loss G: 2.3623, D(x): 0.9058\n",
      "Epoch [21/50] Batch 600/938              Loss D: 0.6723, Loss G: 2.3744, D(x): 0.8943\n",
      "Epoch [21/50] Batch 700/938              Loss D: 0.6762, Loss G: 2.6856, D(x): 0.9324\n",
      "Epoch [21/50] Batch 800/938              Loss D: 0.6793, Loss G: 2.7061, D(x): 0.9241\n",
      "Epoch [21/50] Batch 900/938              Loss D: 0.6575, Loss G: 2.4224, D(x): 0.9004\n",
      "Epoch [22/50] Batch 0/938              Loss D: 0.6641, Loss G: 2.0398, D(x): 0.8706\n",
      "Epoch [22/50] Batch 100/938              Loss D: 0.6686, Loss G: 2.4790, D(x): 0.9323\n",
      "Epoch [22/50] Batch 200/938              Loss D: 0.6640, Loss G: 2.1866, D(x): 0.8843\n",
      "Epoch [22/50] Batch 300/938              Loss D: 0.7067, Loss G: 2.4495, D(x): 0.9006\n",
      "Epoch [22/50] Batch 400/938              Loss D: 0.6679, Loss G: 2.1242, D(x): 0.8679\n",
      "Epoch [22/50] Batch 500/938              Loss D: 0.7004, Loss G: 2.5299, D(x): 0.9404\n",
      "Epoch [22/50] Batch 600/938              Loss D: 0.7240, Loss G: 2.8348, D(x): 0.9377\n",
      "Epoch [22/50] Batch 700/938              Loss D: 0.6745, Loss G: 2.1602, D(x): 0.8601\n",
      "Epoch [22/50] Batch 800/938              Loss D: 0.6699, Loss G: 2.0887, D(x): 0.8568\n",
      "Epoch [22/50] Batch 900/938              Loss D: 1.5865, Loss G: 3.1818, D(x): 0.2793\n",
      "Epoch [23/50] Batch 0/938              Loss D: 0.6953, Loss G: 2.2915, D(x): 0.9107\n",
      "Epoch [23/50] Batch 100/938              Loss D: 0.6899, Loss G: 1.9364, D(x): 0.8191\n",
      "Epoch [23/50] Batch 200/938              Loss D: 0.6609, Loss G: 2.5802, D(x): 0.8932\n",
      "Epoch [23/50] Batch 300/938              Loss D: 0.6709, Loss G: 2.0234, D(x): 0.8581\n",
      "Epoch [23/50] Batch 400/938              Loss D: 0.6623, Loss G: 2.1376, D(x): 0.8907\n",
      "Epoch [23/50] Batch 500/938              Loss D: 0.6642, Loss G: 2.6106, D(x): 0.9218\n",
      "Epoch [23/50] Batch 600/938              Loss D: 0.6645, Loss G: 2.1688, D(x): 0.8630\n",
      "Epoch [23/50] Batch 700/938              Loss D: 0.6594, Loss G: 2.3546, D(x): 0.9227\n",
      "Epoch [23/50] Batch 800/938              Loss D: 0.6966, Loss G: 2.7824, D(x): 0.9401\n",
      "Epoch [23/50] Batch 900/938              Loss D: 0.6590, Loss G: 2.1799, D(x): 0.8886\n",
      "Epoch [24/50] Batch 0/938              Loss D: 0.7321, Loss G: 3.2922, D(x): 0.9517\n",
      "Epoch [24/50] Batch 100/938              Loss D: 0.6601, Loss G: 2.3694, D(x): 0.8854\n",
      "Epoch [24/50] Batch 200/938              Loss D: 0.6612, Loss G: 2.3567, D(x): 0.8878\n",
      "Epoch [24/50] Batch 300/938              Loss D: 0.6632, Loss G: 2.7369, D(x): 0.9262\n",
      "Epoch [24/50] Batch 400/938              Loss D: 0.6715, Loss G: 2.1448, D(x): 0.8623\n",
      "Epoch [24/50] Batch 500/938              Loss D: 0.6595, Loss G: 2.3428, D(x): 0.9037\n",
      "Epoch [24/50] Batch 600/938              Loss D: 0.6705, Loss G: 2.8304, D(x): 0.9308\n",
      "Epoch [24/50] Batch 700/938              Loss D: 0.6582, Loss G: 2.5210, D(x): 0.9098\n",
      "Epoch [24/50] Batch 800/938              Loss D: 0.6780, Loss G: 2.6421, D(x): 0.9289\n",
      "Epoch [24/50] Batch 900/938              Loss D: 0.6572, Loss G: 2.4078, D(x): 0.8986\n",
      "Epoch [25/50] Batch 0/938              Loss D: 0.6589, Loss G: 2.3690, D(x): 0.9143\n",
      "Epoch [25/50] Batch 100/938              Loss D: 0.6642, Loss G: 2.4332, D(x): 0.9172\n",
      "Epoch [25/50] Batch 200/938              Loss D: 1.2359, Loss G: 0.8049, D(x): 0.4381\n",
      "Epoch [25/50] Batch 300/938              Loss D: 0.6765, Loss G: 2.2450, D(x): 0.8902\n",
      "Epoch [25/50] Batch 400/938              Loss D: 0.6641, Loss G: 2.0711, D(x): 0.8843\n",
      "Epoch [25/50] Batch 500/938              Loss D: 0.6661, Loss G: 2.0947, D(x): 0.8579\n",
      "Epoch [25/50] Batch 600/938              Loss D: 0.6895, Loss G: 1.7255, D(x): 0.8070\n",
      "Epoch [25/50] Batch 700/938              Loss D: 0.6633, Loss G: 2.6227, D(x): 0.9108\n",
      "Epoch [25/50] Batch 800/938              Loss D: 0.6612, Loss G: 2.4833, D(x): 0.8897\n",
      "Epoch [25/50] Batch 900/938              Loss D: 0.6608, Loss G: 2.5567, D(x): 0.9013\n",
      "Epoch [26/50] Batch 0/938              Loss D: 0.6779, Loss G: 2.4892, D(x): 0.8881\n",
      "Epoch [26/50] Batch 100/938              Loss D: 0.6703, Loss G: 1.9940, D(x): 0.8452\n",
      "Epoch [26/50] Batch 200/938              Loss D: 0.6573, Loss G: 2.4217, D(x): 0.9098\n",
      "Epoch [26/50] Batch 300/938              Loss D: 0.6663, Loss G: 2.1905, D(x): 0.8658\n",
      "Epoch [26/50] Batch 400/938              Loss D: 1.1995, Loss G: 1.0284, D(x): 0.4824\n",
      "Epoch [26/50] Batch 500/938              Loss D: 0.7065, Loss G: 2.5090, D(x): 0.8985\n",
      "Epoch [26/50] Batch 600/938              Loss D: 0.6607, Loss G: 2.1673, D(x): 0.8874\n",
      "Epoch [26/50] Batch 700/938              Loss D: 0.6772, Loss G: 2.0128, D(x): 0.8370\n",
      "Epoch [26/50] Batch 800/938              Loss D: 0.6622, Loss G: 2.2511, D(x): 0.8911\n",
      "Epoch [26/50] Batch 900/938              Loss D: 0.6590, Loss G: 2.4825, D(x): 0.9118\n",
      "Epoch [27/50] Batch 0/938              Loss D: 0.6800, Loss G: 2.6780, D(x): 0.9255\n",
      "Epoch [27/50] Batch 100/938              Loss D: 0.6728, Loss G: 2.6946, D(x): 0.9188\n",
      "Epoch [27/50] Batch 200/938              Loss D: 0.6598, Loss G: 2.4207, D(x): 0.8931\n",
      "Epoch [27/50] Batch 300/938              Loss D: 0.6600, Loss G: 2.1013, D(x): 0.8978\n",
      "Epoch [27/50] Batch 400/938              Loss D: 0.6594, Loss G: 2.3203, D(x): 0.9058\n",
      "Epoch [27/50] Batch 500/938              Loss D: 0.6567, Loss G: 2.2626, D(x): 0.9019\n",
      "Epoch [27/50] Batch 600/938              Loss D: 0.6576, Loss G: 2.3307, D(x): 0.8870\n",
      "Epoch [27/50] Batch 700/938              Loss D: 0.6620, Loss G: 2.1109, D(x): 0.8644\n",
      "Epoch [27/50] Batch 800/938              Loss D: 0.6583, Loss G: 2.3194, D(x): 0.8854\n",
      "Epoch [27/50] Batch 900/938              Loss D: 0.6810, Loss G: 2.8168, D(x): 0.9297\n",
      "Epoch [28/50] Batch 0/938              Loss D: 0.6620, Loss G: 2.3188, D(x): 0.9209\n",
      "Epoch [28/50] Batch 100/938              Loss D: 0.6594, Loss G: 2.5214, D(x): 0.8888\n",
      "Epoch [28/50] Batch 200/938              Loss D: 0.6587, Loss G: 2.3608, D(x): 0.8978\n",
      "Epoch [28/50] Batch 300/938              Loss D: 0.7602, Loss G: 2.3110, D(x): 0.8892\n",
      "Epoch [28/50] Batch 400/938              Loss D: 0.6621, Loss G: 2.3691, D(x): 0.8881\n",
      "Epoch [28/50] Batch 500/938              Loss D: 0.6634, Loss G: 2.3661, D(x): 0.8711\n",
      "Epoch [28/50] Batch 600/938              Loss D: 0.6588, Loss G: 2.5218, D(x): 0.9087\n",
      "Epoch [28/50] Batch 700/938              Loss D: 0.6597, Loss G: 2.3886, D(x): 0.9205\n",
      "Epoch [28/50] Batch 800/938              Loss D: 0.6690, Loss G: 2.0247, D(x): 0.8491\n",
      "Epoch [28/50] Batch 900/938              Loss D: 0.6622, Loss G: 2.7101, D(x): 0.9236\n",
      "Epoch [29/50] Batch 0/938              Loss D: 0.6684, Loss G: 2.6473, D(x): 0.9344\n",
      "Epoch [29/50] Batch 100/938              Loss D: 0.6637, Loss G: 2.1342, D(x): 0.8617\n",
      "Epoch [29/50] Batch 200/938              Loss D: 0.9190, Loss G: 2.1103, D(x): 0.8180\n",
      "Epoch [29/50] Batch 300/938              Loss D: 0.6823, Loss G: 2.1960, D(x): 0.8315\n",
      "Epoch [29/50] Batch 400/938              Loss D: 0.6661, Loss G: 2.0860, D(x): 0.8653\n",
      "Epoch [29/50] Batch 500/938              Loss D: 0.6701, Loss G: 2.0307, D(x): 0.8527\n",
      "Epoch [29/50] Batch 600/938              Loss D: 0.6809, Loss G: 2.0206, D(x): 0.8235\n",
      "Epoch [29/50] Batch 700/938              Loss D: 0.6747, Loss G: 2.0808, D(x): 0.8338\n",
      "Epoch [29/50] Batch 800/938              Loss D: 0.6762, Loss G: 1.9157, D(x): 0.8305\n",
      "Epoch [29/50] Batch 900/938              Loss D: 0.6687, Loss G: 2.0887, D(x): 0.8475\n",
      "Epoch [30/50] Batch 0/938              Loss D: 0.6561, Loss G: 2.3006, D(x): 0.8952\n",
      "Epoch [30/50] Batch 100/938              Loss D: 0.6578, Loss G: 2.4458, D(x): 0.8879\n",
      "Epoch [30/50] Batch 200/938              Loss D: 0.6561, Loss G: 2.2323, D(x): 0.8856\n",
      "Epoch [30/50] Batch 300/938              Loss D: 0.6550, Loss G: 2.4388, D(x): 0.9044\n",
      "Epoch [30/50] Batch 400/938              Loss D: 0.6681, Loss G: 2.6281, D(x): 0.8990\n",
      "Epoch [30/50] Batch 500/938              Loss D: 0.6595, Loss G: 2.5162, D(x): 0.9033\n",
      "Epoch [30/50] Batch 600/938              Loss D: 0.6646, Loss G: 2.1232, D(x): 0.8725\n",
      "Epoch [30/50] Batch 700/938              Loss D: 0.6608, Loss G: 2.1494, D(x): 0.8786\n",
      "Epoch [30/50] Batch 800/938              Loss D: 0.6566, Loss G: 2.4688, D(x): 0.8919\n",
      "Epoch [30/50] Batch 900/938              Loss D: 0.6618, Loss G: 2.2857, D(x): 0.9032\n",
      "Epoch [31/50] Batch 0/938              Loss D: 0.6578, Loss G: 2.3857, D(x): 0.8849\n",
      "Epoch [31/50] Batch 100/938              Loss D: 0.6697, Loss G: 2.6119, D(x): 0.9181\n",
      "Epoch [31/50] Batch 200/938              Loss D: 0.6582, Loss G: 2.1556, D(x): 0.8881\n",
      "Epoch [31/50] Batch 300/938              Loss D: 0.6921, Loss G: 2.5321, D(x): 0.9433\n",
      "Epoch [31/50] Batch 400/938              Loss D: 0.6621, Loss G: 2.5251, D(x): 0.9285\n",
      "Epoch [31/50] Batch 500/938              Loss D: 0.6579, Loss G: 2.4914, D(x): 0.9205\n",
      "Epoch [31/50] Batch 600/938              Loss D: 0.6617, Loss G: 2.3982, D(x): 0.9238\n",
      "Epoch [31/50] Batch 700/938              Loss D: 0.6576, Loss G: 2.2289, D(x): 0.8906\n",
      "Epoch [31/50] Batch 800/938              Loss D: 0.6639, Loss G: 2.3949, D(x): 0.8781\n",
      "Epoch [31/50] Batch 900/938              Loss D: 0.6682, Loss G: 2.2365, D(x): 0.8588\n",
      "Epoch [32/50] Batch 0/938              Loss D: 0.6625, Loss G: 2.6044, D(x): 0.9035\n",
      "Epoch [32/50] Batch 100/938              Loss D: 0.6589, Loss G: 2.2663, D(x): 0.8739\n",
      "Epoch [32/50] Batch 200/938              Loss D: 0.6556, Loss G: 2.2588, D(x): 0.8865\n",
      "Epoch [32/50] Batch 300/938              Loss D: 0.7221, Loss G: 3.0731, D(x): 0.9427\n",
      "Epoch [32/50] Batch 400/938              Loss D: 0.6633, Loss G: 2.1025, D(x): 0.8659\n",
      "Epoch [32/50] Batch 500/938              Loss D: 0.6584, Loss G: 2.2703, D(x): 0.9086\n",
      "Epoch [32/50] Batch 600/938              Loss D: 0.6756, Loss G: 2.6622, D(x): 0.9312\n",
      "Epoch [32/50] Batch 700/938              Loss D: 0.6627, Loss G: 2.4119, D(x): 0.9092\n",
      "Epoch [32/50] Batch 800/938              Loss D: 0.6610, Loss G: 2.1600, D(x): 0.8798\n",
      "Epoch [32/50] Batch 900/938              Loss D: 0.6570, Loss G: 2.4924, D(x): 0.8975\n",
      "Epoch [33/50] Batch 0/938              Loss D: 0.7960, Loss G: 1.4204, D(x): 0.7365\n",
      "Epoch [33/50] Batch 100/938              Loss D: 0.6599, Loss G: 2.2577, D(x): 0.8951\n",
      "Epoch [33/50] Batch 200/938              Loss D: 0.6582, Loss G: 2.4593, D(x): 0.9069\n",
      "Epoch [33/50] Batch 300/938              Loss D: 0.6626, Loss G: 2.0363, D(x): 0.8579\n",
      "Epoch [33/50] Batch 400/938              Loss D: 0.6620, Loss G: 2.2559, D(x): 0.9012\n",
      "Epoch [33/50] Batch 500/938              Loss D: 0.6576, Loss G: 2.2470, D(x): 0.8814\n",
      "Epoch [33/50] Batch 600/938              Loss D: 0.6675, Loss G: 2.4333, D(x): 0.9200\n",
      "Epoch [33/50] Batch 700/938              Loss D: 0.6633, Loss G: 2.1657, D(x): 0.8674\n",
      "Epoch [33/50] Batch 800/938              Loss D: 0.6588, Loss G: 2.2821, D(x): 0.9136\n",
      "Epoch [33/50] Batch 900/938              Loss D: 0.6565, Loss G: 2.3675, D(x): 0.8935\n",
      "Epoch [34/50] Batch 0/938              Loss D: 0.6651, Loss G: 2.0825, D(x): 0.8622\n",
      "Epoch [34/50] Batch 100/938              Loss D: 0.6716, Loss G: 2.5869, D(x): 0.9274\n",
      "Epoch [34/50] Batch 200/938              Loss D: 0.6563, Loss G: 2.3082, D(x): 0.8954\n",
      "Epoch [34/50] Batch 300/938              Loss D: 0.6566, Loss G: 2.5868, D(x): 0.9082\n",
      "Epoch [34/50] Batch 400/938              Loss D: 0.6607, Loss G: 2.3367, D(x): 0.8661\n",
      "Epoch [34/50] Batch 500/938              Loss D: 0.6669, Loss G: 2.0628, D(x): 0.8511\n",
      "Epoch [34/50] Batch 600/938              Loss D: 0.6563, Loss G: 2.2486, D(x): 0.9045\n",
      "Epoch [34/50] Batch 700/938              Loss D: 0.6702, Loss G: 2.1258, D(x): 0.8374\n",
      "Epoch [34/50] Batch 800/938              Loss D: 0.6696, Loss G: 2.4382, D(x): 0.8869\n",
      "Epoch [34/50] Batch 900/938              Loss D: 0.6539, Loss G: 2.3183, D(x): 0.9079\n",
      "Epoch [35/50] Batch 0/938              Loss D: 0.7088, Loss G: 2.3565, D(x): 0.8326\n",
      "Epoch [35/50] Batch 100/938              Loss D: 0.6576, Loss G: 2.2005, D(x): 0.8778\n",
      "Epoch [35/50] Batch 200/938              Loss D: 0.6623, Loss G: 2.6326, D(x): 0.9339\n",
      "Epoch [35/50] Batch 300/938              Loss D: 0.6575, Loss G: 2.3347, D(x): 0.9108\n",
      "Epoch [35/50] Batch 400/938              Loss D: 0.6737, Loss G: 2.0087, D(x): 0.8421\n",
      "Epoch [35/50] Batch 500/938              Loss D: 0.6866, Loss G: 2.9435, D(x): 0.8859\n",
      "Epoch [35/50] Batch 600/938              Loss D: 0.6899, Loss G: 3.2492, D(x): 0.9490\n",
      "Epoch [35/50] Batch 700/938              Loss D: 0.6553, Loss G: 2.4238, D(x): 0.9041\n",
      "Epoch [35/50] Batch 800/938              Loss D: 0.6752, Loss G: 2.8108, D(x): 0.9145\n",
      "Epoch [35/50] Batch 900/938              Loss D: 0.6723, Loss G: 2.6449, D(x): 0.9456\n",
      "Epoch [36/50] Batch 0/938              Loss D: 0.6595, Loss G: 2.3421, D(x): 0.9269\n",
      "Epoch [36/50] Batch 100/938              Loss D: 0.6962, Loss G: 2.8741, D(x): 0.9458\n",
      "Epoch [36/50] Batch 200/938              Loss D: 0.6544, Loss G: 2.3169, D(x): 0.9000\n",
      "Epoch [36/50] Batch 300/938              Loss D: 0.6670, Loss G: 2.1034, D(x): 0.8581\n",
      "Epoch [36/50] Batch 400/938              Loss D: 0.6637, Loss G: 2.4869, D(x): 0.9265\n",
      "Epoch [36/50] Batch 500/938              Loss D: 0.6577, Loss G: 2.4109, D(x): 0.9056\n",
      "Epoch [36/50] Batch 600/938              Loss D: 0.6558, Loss G: 2.3769, D(x): 0.9159\n",
      "Epoch [36/50] Batch 700/938              Loss D: 1.2244, Loss G: 1.1220, D(x): 0.5299\n",
      "Epoch [36/50] Batch 800/938              Loss D: 0.7356, Loss G: 1.2619, D(x): 0.7491\n",
      "Epoch [36/50] Batch 900/938              Loss D: 0.6701, Loss G: 2.3046, D(x): 0.9087\n",
      "Epoch [37/50] Batch 0/938              Loss D: 0.6576, Loss G: 2.2574, D(x): 0.9094\n",
      "Epoch [37/50] Batch 100/938              Loss D: 0.6591, Loss G: 2.2549, D(x): 0.8768\n",
      "Epoch [37/50] Batch 200/938              Loss D: 0.6584, Loss G: 2.2957, D(x): 0.8817\n",
      "Epoch [37/50] Batch 300/938              Loss D: 0.6560, Loss G: 2.3409, D(x): 0.8961\n",
      "Epoch [37/50] Batch 400/938              Loss D: 0.6621, Loss G: 2.2567, D(x): 0.8637\n",
      "Epoch [37/50] Batch 500/938              Loss D: 0.6573, Loss G: 2.2533, D(x): 0.8967\n",
      "Epoch [37/50] Batch 600/938              Loss D: 0.6603, Loss G: 1.9797, D(x): 0.8646\n",
      "Epoch [37/50] Batch 700/938              Loss D: 0.6647, Loss G: 2.4244, D(x): 0.9251\n",
      "Epoch [37/50] Batch 800/938              Loss D: 0.6553, Loss G: 2.4117, D(x): 0.8989\n",
      "Epoch [37/50] Batch 900/938              Loss D: 0.6638, Loss G: 2.3461, D(x): 0.8948\n",
      "Epoch [38/50] Batch 0/938              Loss D: 0.6560, Loss G: 2.2221, D(x): 0.8859\n",
      "Epoch [38/50] Batch 100/938              Loss D: 0.6544, Loss G: 2.2700, D(x): 0.9040\n",
      "Epoch [38/50] Batch 200/938              Loss D: 0.6541, Loss G: 2.2388, D(x): 0.8910\n",
      "Epoch [38/50] Batch 300/938              Loss D: 0.6557, Loss G: 2.1880, D(x): 0.8853\n",
      "Epoch [38/50] Batch 400/938              Loss D: 0.6604, Loss G: 2.1909, D(x): 0.8670\n",
      "Epoch [38/50] Batch 500/938              Loss D: 0.6699, Loss G: 2.5498, D(x): 0.9416\n",
      "Epoch [38/50] Batch 600/938              Loss D: 0.6549, Loss G: 2.1880, D(x): 0.8854\n",
      "Epoch [38/50] Batch 700/938              Loss D: 0.6554, Loss G: 2.0346, D(x): 0.8944\n",
      "Epoch [38/50] Batch 800/938              Loss D: 0.6543, Loss G: 2.3975, D(x): 0.9091\n",
      "Epoch [38/50] Batch 900/938              Loss D: 0.7047, Loss G: 2.5914, D(x): 0.9255\n",
      "Epoch [39/50] Batch 0/938              Loss D: 0.6963, Loss G: 2.6182, D(x): 0.9393\n",
      "Epoch [39/50] Batch 100/938              Loss D: 0.6842, Loss G: 2.6456, D(x): 0.9303\n",
      "Epoch [39/50] Batch 200/938              Loss D: 0.6762, Loss G: 2.3381, D(x): 0.8519\n",
      "Epoch [39/50] Batch 300/938              Loss D: 0.6596, Loss G: 2.4426, D(x): 0.8876\n",
      "Epoch [39/50] Batch 400/938              Loss D: 0.6553, Loss G: 2.3074, D(x): 0.9016\n",
      "Epoch [39/50] Batch 500/938              Loss D: 0.6592, Loss G: 2.6647, D(x): 0.9212\n",
      "Epoch [39/50] Batch 600/938              Loss D: 0.6574, Loss G: 2.1729, D(x): 0.8843\n",
      "Epoch [39/50] Batch 700/938              Loss D: 0.6782, Loss G: 2.6512, D(x): 0.9309\n",
      "Epoch [39/50] Batch 800/938              Loss D: 0.6556, Loss G: 2.3610, D(x): 0.9034\n",
      "Epoch [39/50] Batch 900/938              Loss D: 0.6599, Loss G: 2.3455, D(x): 0.9099\n",
      "Epoch [40/50] Batch 0/938              Loss D: 0.6794, Loss G: 2.0509, D(x): 0.8542\n",
      "Epoch [40/50] Batch 100/938              Loss D: 0.6625, Loss G: 2.2277, D(x): 0.9000\n",
      "Epoch [40/50] Batch 200/938              Loss D: 0.6637, Loss G: 1.8847, D(x): 0.8590\n",
      "Epoch [40/50] Batch 300/938              Loss D: 0.6583, Loss G: 2.2566, D(x): 0.9143\n",
      "Epoch [40/50] Batch 400/938              Loss D: 0.6722, Loss G: 2.3290, D(x): 0.9081\n",
      "Epoch [40/50] Batch 500/938              Loss D: 0.6552, Loss G: 2.3919, D(x): 0.8911\n",
      "Epoch [40/50] Batch 600/938              Loss D: 0.6541, Loss G: 2.2597, D(x): 0.9079\n",
      "Epoch [40/50] Batch 700/938              Loss D: 0.6626, Loss G: 2.5457, D(x): 0.9261\n",
      "Epoch [40/50] Batch 800/938              Loss D: 0.6589, Loss G: 2.5233, D(x): 0.8918\n",
      "Epoch [40/50] Batch 900/938              Loss D: 0.6538, Loss G: 2.2927, D(x): 0.8896\n",
      "Epoch [41/50] Batch 0/938              Loss D: 0.6536, Loss G: 2.2801, D(x): 0.8935\n",
      "Epoch [41/50] Batch 100/938              Loss D: 0.6576, Loss G: 2.1006, D(x): 0.8658\n",
      "Epoch [41/50] Batch 200/938              Loss D: 0.6610, Loss G: 2.3172, D(x): 0.8906\n",
      "Epoch [41/50] Batch 300/938              Loss D: 0.6702, Loss G: 2.5836, D(x): 0.9286\n",
      "Epoch [41/50] Batch 400/938              Loss D: 0.6540, Loss G: 2.1171, D(x): 0.9147\n",
      "Epoch [41/50] Batch 500/938              Loss D: 0.6565, Loss G: 2.3758, D(x): 0.8728\n",
      "Epoch [41/50] Batch 600/938              Loss D: 0.6548, Loss G: 2.3808, D(x): 0.8851\n",
      "Epoch [41/50] Batch 700/938              Loss D: 0.6612, Loss G: 2.1642, D(x): 0.8657\n",
      "Epoch [41/50] Batch 800/938              Loss D: 0.6574, Loss G: 2.2074, D(x): 0.8802\n",
      "Epoch [41/50] Batch 900/938              Loss D: 0.6546, Loss G: 2.3744, D(x): 0.9160\n",
      "Epoch [42/50] Batch 0/938              Loss D: 0.6680, Loss G: 2.4025, D(x): 0.9078\n",
      "Epoch [42/50] Batch 100/938              Loss D: 0.6646, Loss G: 2.1457, D(x): 0.8624\n",
      "Epoch [42/50] Batch 200/938              Loss D: 0.6552, Loss G: 2.3910, D(x): 0.8987\n",
      "Epoch [42/50] Batch 300/938              Loss D: 0.6782, Loss G: 2.4316, D(x): 0.8786\n",
      "Epoch [42/50] Batch 400/938              Loss D: 0.6573, Loss G: 2.3845, D(x): 0.8966\n",
      "Epoch [42/50] Batch 500/938              Loss D: 0.6536, Loss G: 2.2235, D(x): 0.8988\n",
      "Epoch [42/50] Batch 600/938              Loss D: 0.6572, Loss G: 2.1735, D(x): 0.8849\n",
      "Epoch [42/50] Batch 700/938              Loss D: 0.6692, Loss G: 2.6802, D(x): 0.9359\n",
      "Epoch [42/50] Batch 800/938              Loss D: 0.6551, Loss G: 2.2310, D(x): 0.9163\n",
      "Epoch [42/50] Batch 900/938              Loss D: 0.6570, Loss G: 2.3056, D(x): 0.8937\n",
      "Epoch [43/50] Batch 0/938              Loss D: 0.6544, Loss G: 2.3822, D(x): 0.9101\n",
      "Epoch [43/50] Batch 100/938              Loss D: 0.6883, Loss G: 2.1997, D(x): 0.8497\n",
      "Epoch [43/50] Batch 200/938              Loss D: 0.6546, Loss G: 2.3038, D(x): 0.8808\n",
      "Epoch [43/50] Batch 300/938              Loss D: 0.6643, Loss G: 2.1938, D(x): 0.8891\n",
      "Epoch [43/50] Batch 400/938              Loss D: 0.6544, Loss G: 2.2043, D(x): 0.8982\n",
      "Epoch [43/50] Batch 500/938              Loss D: 0.6684, Loss G: 2.5930, D(x): 0.9273\n",
      "Epoch [43/50] Batch 600/938              Loss D: 0.6569, Loss G: 2.4068, D(x): 0.8955\n",
      "Epoch [43/50] Batch 700/938              Loss D: 0.6548, Loss G: 2.4388, D(x): 0.8989\n",
      "Epoch [43/50] Batch 800/938              Loss D: 0.6540, Loss G: 2.3871, D(x): 0.9127\n",
      "Epoch [43/50] Batch 900/938              Loss D: 0.6550, Loss G: 2.3883, D(x): 0.8966\n",
      "Epoch [44/50] Batch 0/938              Loss D: 0.7050, Loss G: 3.1224, D(x): 0.9382\n",
      "Epoch [44/50] Batch 100/938              Loss D: 0.6539, Loss G: 2.2655, D(x): 0.9021\n",
      "Epoch [44/50] Batch 200/938              Loss D: 0.6563, Loss G: 2.2007, D(x): 0.8736\n",
      "Epoch [44/50] Batch 300/938              Loss D: 0.6656, Loss G: 2.5081, D(x): 0.9269\n",
      "Epoch [44/50] Batch 400/938              Loss D: 0.6614, Loss G: 1.9105, D(x): 0.8599\n",
      "Epoch [44/50] Batch 500/938              Loss D: 0.6551, Loss G: 2.2118, D(x): 0.8815\n",
      "Epoch [44/50] Batch 600/938              Loss D: 0.6551, Loss G: 2.0926, D(x): 0.8775\n",
      "Epoch [44/50] Batch 700/938              Loss D: 0.6562, Loss G: 2.3580, D(x): 0.8792\n",
      "Epoch [44/50] Batch 800/938              Loss D: 0.6702, Loss G: 2.0130, D(x): 0.8389\n",
      "Epoch [44/50] Batch 900/938              Loss D: 0.6626, Loss G: 2.1504, D(x): 0.8570\n",
      "Epoch [45/50] Batch 0/938              Loss D: 0.6716, Loss G: 2.5833, D(x): 0.9036\n",
      "Epoch [45/50] Batch 100/938              Loss D: 0.6541, Loss G: 2.4148, D(x): 0.9145\n",
      "Epoch [45/50] Batch 200/938              Loss D: 0.6606, Loss G: 2.3308, D(x): 0.8869\n",
      "Epoch [45/50] Batch 300/938              Loss D: 0.6573, Loss G: 2.4419, D(x): 0.9168\n",
      "Epoch [45/50] Batch 400/938              Loss D: 0.6685, Loss G: 1.8199, D(x): 0.8395\n",
      "Epoch [45/50] Batch 500/938              Loss D: 0.6828, Loss G: 2.3590, D(x): 0.8339\n",
      "Epoch [45/50] Batch 600/938              Loss D: 0.6568, Loss G: 2.3803, D(x): 0.8981\n",
      "Epoch [45/50] Batch 700/938              Loss D: 0.6617, Loss G: 2.0492, D(x): 0.8681\n",
      "Epoch [45/50] Batch 800/938              Loss D: 0.6563, Loss G: 2.3485, D(x): 0.9119\n",
      "Epoch [45/50] Batch 900/938              Loss D: 0.6535, Loss G: 2.2047, D(x): 0.8946\n",
      "Epoch [46/50] Batch 0/938              Loss D: 0.6543, Loss G: 2.3142, D(x): 0.9112\n",
      "Epoch [46/50] Batch 100/938              Loss D: 0.6580, Loss G: 2.4179, D(x): 0.8912\n",
      "Epoch [46/50] Batch 200/938              Loss D: 0.6564, Loss G: 2.4907, D(x): 0.9184\n",
      "Epoch [46/50] Batch 300/938              Loss D: 0.6551, Loss G: 2.3182, D(x): 0.8869\n",
      "Epoch [46/50] Batch 400/938              Loss D: 0.6531, Loss G: 2.3219, D(x): 0.8947\n",
      "Epoch [46/50] Batch 500/938              Loss D: 0.6651, Loss G: 2.2637, D(x): 0.8518\n",
      "Epoch [46/50] Batch 600/938              Loss D: 0.6546, Loss G: 2.2566, D(x): 0.8917\n",
      "Epoch [46/50] Batch 700/938              Loss D: 0.6606, Loss G: 2.4721, D(x): 0.8854\n",
      "Epoch [46/50] Batch 800/938              Loss D: 0.6589, Loss G: 2.2518, D(x): 0.8613\n",
      "Epoch [46/50] Batch 900/938              Loss D: 0.6535, Loss G: 2.5145, D(x): 0.9091\n",
      "Epoch [47/50] Batch 0/938              Loss D: 0.6538, Loss G: 2.2371, D(x): 0.9138\n",
      "Epoch [47/50] Batch 100/938              Loss D: 0.6718, Loss G: 2.6217, D(x): 0.9299\n",
      "Epoch [47/50] Batch 200/938              Loss D: 0.6545, Loss G: 2.4624, D(x): 0.9190\n",
      "Epoch [47/50] Batch 300/938              Loss D: 0.6678, Loss G: 2.5012, D(x): 0.9327\n",
      "Epoch [47/50] Batch 400/938              Loss D: 0.8263, Loss G: 1.2528, D(x): 0.6599\n",
      "Epoch [47/50] Batch 500/938              Loss D: 0.6572, Loss G: 2.5508, D(x): 0.9085\n",
      "Epoch [47/50] Batch 600/938              Loss D: 0.6535, Loss G: 2.2340, D(x): 0.8945\n",
      "Epoch [47/50] Batch 700/938              Loss D: 0.6685, Loss G: 1.9556, D(x): 0.8476\n",
      "Epoch [47/50] Batch 800/938              Loss D: 0.6713, Loss G: 2.3794, D(x): 0.9155\n",
      "Epoch [47/50] Batch 900/938              Loss D: 0.6547, Loss G: 2.4250, D(x): 0.8973\n",
      "Epoch [48/50] Batch 0/938              Loss D: 0.6703, Loss G: 2.2246, D(x): 0.8499\n",
      "Epoch [48/50] Batch 100/938              Loss D: 0.6570, Loss G: 2.1560, D(x): 0.8945\n",
      "Epoch [48/50] Batch 200/938              Loss D: 0.6554, Loss G: 2.2848, D(x): 0.8933\n",
      "Epoch [48/50] Batch 300/938              Loss D: 0.6580, Loss G: 2.2582, D(x): 0.8950\n",
      "Epoch [48/50] Batch 400/938              Loss D: 0.6525, Loss G: 2.3503, D(x): 0.8951\n",
      "Epoch [48/50] Batch 500/938              Loss D: 0.6549, Loss G: 2.4171, D(x): 0.8985\n",
      "Epoch [48/50] Batch 600/938              Loss D: 0.6629, Loss G: 2.2143, D(x): 0.8691\n",
      "Epoch [48/50] Batch 700/938              Loss D: 0.6587, Loss G: 2.2295, D(x): 0.8714\n",
      "Epoch [48/50] Batch 800/938              Loss D: 0.6560, Loss G: 2.3355, D(x): 0.8819\n",
      "Epoch [48/50] Batch 900/938              Loss D: 0.6587, Loss G: 2.2140, D(x): 0.8657\n",
      "Epoch [49/50] Batch 0/938              Loss D: 0.6558, Loss G: 2.4130, D(x): 0.9064\n",
      "Epoch [49/50] Batch 100/938              Loss D: 0.6536, Loss G: 2.3992, D(x): 0.8953\n",
      "Epoch [49/50] Batch 200/938              Loss D: 0.6589, Loss G: 2.3456, D(x): 0.8815\n",
      "Epoch [49/50] Batch 300/938              Loss D: 0.6540, Loss G: 2.3365, D(x): 0.8995\n",
      "Epoch [49/50] Batch 400/938              Loss D: 0.6581, Loss G: 2.5536, D(x): 0.9295\n",
      "Epoch [49/50] Batch 500/938              Loss D: 0.6563, Loss G: 2.0805, D(x): 0.8819\n",
      "Epoch [49/50] Batch 600/938              Loss D: 0.6563, Loss G: 2.5060, D(x): 0.9238\n",
      "Epoch [49/50] Batch 700/938              Loss D: 0.6524, Loss G: 2.2810, D(x): 0.8950\n",
      "Epoch [49/50] Batch 800/938              Loss D: 0.6710, Loss G: 2.1548, D(x): 0.8486\n",
      "Epoch [49/50] Batch 900/938              Loss D: 0.6555, Loss G: 2.1866, D(x): 0.8779\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:07.113351Z",
     "start_time": "2024-10-21T22:39:06.684073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained Inception v3 model\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).eval().to(device)"
   ],
   "id": "b4bbdb6199610e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meddi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\meddi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:07.362786Z",
     "start_time": "2024-10-21T22:39:07.323887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_inception_score(rgb_images, inception_model, splits=10):\n",
    "    # rgb_images = images.repeat(1, 3, 1, 1) # not necessary since I'm doing that before\n",
    "    \n",
    "    rgb_images_resized = F.interpolate(rgb_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = inception_model(rgb_images_resized).softmax(dim=1)\n",
    "        \n",
    "    split_scores = []\n",
    "    for i in range(splits):\n",
    "        part = preds[i * (len(preds) // splits): ((i + 1) * len(preds) // splits), :]\n",
    "        p_y = part.mean(dim=0)\n",
    "        split_scores.append(torch.exp((part * (part.log() - p_y.log())).sum(dim=1).mean()))\n",
    "        \n",
    "    return torch.mean(torch.tensor(split_scores)), torch.std(torch.tensor(split_scores))"
   ],
   "id": "35da5b642c9af803",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:07.710980Z",
     "start_time": "2024-10-21T22:39:07.692843Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "ba2f88762984cfb0",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:08.004462Z",
     "start_time": "2024-10-21T22:39:07.989755Z"
    }
   },
   "cell_type": "code",
   "source": "from scipy import linalg",
   "id": "42ee06839d178a5e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:08.503184Z",
     "start_time": "2024-10-21T22:39:08.298343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_images = 32\n",
    "\n",
    "first_batch = next(iter(my_dataloader))\n",
    "\n",
    "real_images = first_batch[0]\n",
    "real_images = real_images[:num_images]\n",
    "real_images = (real_images + 1) / 2\n",
    "rgb_real_images = real_images.repeat(1, 3, 1, 1)\n",
    "rgb_real_images = rgb_real_images.to(device)\n",
    "print(\"rgb_real_images shape: \", rgb_real_images.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(num_images, channels_noise, 1, 1).to(device)\n",
    "    fake_images = netG(noise).to(device)\n",
    "\n",
    "    fake_images = (fake_images + 1) / 2\n",
    "    rgb_fake_images = fake_images.repeat(1, 3, 1, 1)\n",
    "    \n",
    "print(\"rgb_fake_images shape, \", rgb_fake_images.shape)\n",
    "    "
   ],
   "id": "46e50507c0b0e18d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_real_images shape:  torch.Size([32, 3, 64, 64])\n",
      "rgb_fake_images shape,  torch.Size([32, 3, 64, 64])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:09.560875Z",
     "start_time": "2024-10-21T22:39:09.538046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_activation_statistics(images, model, dims=2048, batch_size=128):\n",
    "    model.eval()\n",
    "    act = np.empty((len(images), dims))\n",
    "    \n",
    "    batch = images.cuda()\n",
    "    \n",
    "    pred = model(batch)[0]\n",
    "    \n",
    "    # if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "    #     # pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "    #     print(\"triggered weird check I don't have a function for\")\n",
    "        \n",
    "    act = pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "    \n",
    "    "
   ],
   "id": "4cb13ae16d2f658",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:10.411706Z",
     "start_time": "2024-10-21T22:39:10.363466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    \n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different legnths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # does something similar where it checks to see if the covmean is valid and same as the positive semi definite\n",
    "    # it adds an epsilon, though at a different point\n",
    "    if not np.isinf(covmean).all():\n",
    "        print(\"frechet distance failed; adding to diagonal of cov estimates\")\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        \n",
    "        covmean, _ = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset), disp=False)\n",
    "        \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "        \n",
    "    tr_covmean = np.trace(covmean)\n",
    "    fid = (diff.dot(diff)) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    return fid"
   ],
   "id": "8387c7ccae11f38b",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:10.936425Z",
     "start_time": "2024-10-21T22:39:10.925084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_fid_score(real_images, fake_images, model):\n",
    "    mu1, sigma1 = calculate_activation_statistics(real_images, model)\n",
    "    mu2, sigma2 = calculate_activation_statistics(fake_images, model)\n",
    "    \n",
    "    fid_value = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return fid_value"
   ],
   "id": "e3f25031b52e29b1",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:11.288194Z",
     "start_time": "2024-10-21T22:39:11.274539Z"
    }
   },
   "cell_type": "code",
   "source": "rgb_real_images.shape",
   "id": "b6849da4f357182b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:11.579557Z",
     "start_time": "2024-10-21T22:39:11.572737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resize_images(images):\n",
    "    return F.interpolate(images, size=(299,  299), mode='bilinear', align_corners=False)"
   ],
   "id": "855a89319157d985",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:11.867103Z",
     "start_time": "2024-10-21T22:39:11.850568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resized_rgb_real_images = resize_images(rgb_real_images)\n",
    "resized_rgb_fake_images = resize_images(rgb_fake_images)\n",
    "\n",
    "print(\"resized rgb_real_images shape: \", resized_rgb_real_images.shape)\n",
    "print(\"resized rgb_fake_images shape: \", resized_rgb_fake_images.shape)"
   ],
   "id": "308d4a29ba9b26bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resized rgb_real_images shape:  torch.Size([32, 3, 299, 299])\n",
      "resized rgb_fake_images shape:  torch.Size([32, 3, 299, 299])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:23.896544Z",
     "start_time": "2024-10-21T22:39:12.279327Z"
    }
   },
   "cell_type": "code",
   "source": "fid_score = calculate_fid_score(resized_rgb_real_images, resized_rgb_fake_images, inception_model)",
   "id": "cd7ed259365cb589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frechet distance failed; adding to diagonal of cov estimates\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:24.104069Z",
     "start_time": "2024-10-21T22:39:24.097129Z"
    }
   },
   "cell_type": "code",
   "source": "fid_score",
   "id": "b6b62520245676ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01880484571625196"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:30.005009Z",
     "start_time": "2024-10-21T22:39:24.324080Z"
    }
   },
   "cell_type": "code",
   "source": "inception_score, is_std = calculate_inception_score(rgb_fake_images, inception_model)",
   "id": "d4db4e7892df46d2",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:30.425133Z",
     "start_time": "2024-10-21T22:39:30.408742Z"
    }
   },
   "cell_type": "code",
   "source": "inception_score",
   "id": "1aa1039d6382edfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5159)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:30.919176Z",
     "start_time": "2024-10-21T22:39:30.904957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np_is_score = inception_score.numpy()\n",
    "\n",
    "\n",
    "with open('../observations/dcgan_experiment_2_observations.txt', \"a\") as file:\n",
    "    file.write(\"Base_dcgan measurements\\n\")\n",
    "    file.write(f\"inception score {np.array2string(np_is_score)}\\n\")\n",
    "    file.write(f\"fid score {np.array2string(fid_score)}\\n\")\n"
   ],
   "id": "f7700e050ed21e7f",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:39:31.590120Z",
     "start_time": "2024-10-21T22:39:31.583817Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8b9609df4de296f7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
