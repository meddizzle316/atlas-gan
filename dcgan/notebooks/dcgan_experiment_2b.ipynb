{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:26.462272Z",
     "start_time": "2024-10-21T21:15:20.831448Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:26.486327Z",
     "start_time": "2024-10-21T21:15:26.481220Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.__version__)",
   "id": "9dc942bfe103e1c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:27.241466Z",
     "start_time": "2024-10-21T21:15:27.161615Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.is_available())",
   "id": "137b405b8ec86171",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:27.336680Z",
     "start_time": "2024-10-21T21:15:27.332972Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.version.cuda)",
   "id": "d152c5800b640b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:27.463940Z",
     "start_time": "2024-10-21T21:15:27.448914Z"
    }
   },
   "cell_type": "code",
   "source": "tensor_cpu = torch.randn(3, 3)",
   "id": "5629e1d8ea31d4b3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:27.676878Z",
     "start_time": "2024-10-21T21:15:27.547421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "tensor_gpu = tensor_cpu.to('cuda')"
   ],
   "id": "c2e24cd9d00a17f5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:28.162779Z",
     "start_time": "2024-10-21T21:15:28.106783Z"
    }
   },
   "cell_type": "code",
   "source": "print(tensor_gpu.device)",
   "id": "54aa193243890f6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.058468Z",
     "start_time": "2024-10-21T21:15:28.221143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as dataloader\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ],
   "id": "d96150f2a15503f9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.156551Z",
     "start_time": "2024-10-21T21:15:45.080825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # is the convention for Conv in pytorch N, channels, height, width?\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), # what does padding 1 correspond to?\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1), # why features_d for filters? Why features_d * 2?\n",
    "            nn.BatchNorm2d(features_d * 2), # because GANS are known for being notoriously unstable during training -- why are GANS known for this?\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features_d *2, features_d * 4, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(features_d * 4), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d * 4, features_d * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_d * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # N x features_d * 8 x 4 x 4\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            \n",
    "            # N x 1 x 1 x 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n"
   ],
   "id": "f86d46c295864940",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.195305Z",
     "start_time": "2024-10-21T21:15:45.182946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # N x channels_noise x 1 x 1\n",
    "            nn.ConvTranspose2d(channels_noise, features_g * 16, kernel_size=4, stride=1, padding=0), \n",
    "            nn.BatchNorm2d(features_g * 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # N x features_g * 16 x 4 x 4\n",
    "            nn.ConvTranspose2d(features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g * 8, features_g* 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            \n",
    "            # N x channels_img # 64 x 64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "8c8407fe0a2f6c65",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.253712Z",
     "start_time": "2024-10-21T21:15:45.243295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = 0.0002\n",
    "batch_size = 32\n",
    "image_size = 64 # 28 x 28 >>> 64x64\n",
    "channels_img = 1\n",
    "channels_noise = 256\n",
    "\n",
    "features_d = 16 # was set at 64 in the paper but not needed for mnist might for celebrity faces though\n",
    "features_g = 16 # was set at 64 in the paper but not needed for mnist might for celebrity faces though"
   ],
   "id": "5fbde815fb0b2678",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.291353Z",
     "start_time": "2024-10-21T21:15:45.283628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_transforms = transforms.Compose([ # what does the transforms do in pytorch??\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ],
   "id": "815cd660a175607c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.442118Z",
     "start_time": "2024-10-21T21:15:45.320762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = datasets.MNIST(root='dataset/', train=True, transform=my_transforms, download=True) # seems to download the specified dataset to my directory\n",
    "my_dataloader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) # not sure what's going on thought I already imported this?\n"
   ],
   "id": "d0f0f89ecd3793b7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.472538Z",
     "start_time": "2024-10-21T21:15:45.465186Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
   "id": "dbc35f8de6c91776",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.580846Z",
     "start_time": "2024-10-21T21:15:45.495626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create discriminator and generator\n",
    "\n",
    "netD = Discriminator(channels_img, features_d).to(device) #  Iguess every model has to be specified to a device? So I could run some on different gpus or my cpu? \n",
    "netG = Generator(channels_noise, channels_img, features_g).to(device)"
   ],
   "id": "305e2080cd6e3b24",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.621816Z",
     "start_time": "2024-10-21T21:15:45.613348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup Optimizer for G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999)) # why are we specifying? What is the default betas value? 0.9 and 0.999?\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
   ],
   "id": "396764a0e16d0f28",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.682298Z",
     "start_time": "2024-10-21T21:15:45.663137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "netD.train() # what is the difference being in training mode from otherwise in pytorch?"
   ],
   "id": "1e043c6a90181cff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.822902Z",
     "start_time": "2024-10-21T21:15:45.812026Z"
    }
   },
   "cell_type": "code",
   "source": "netG.train() # apparently the models should be in training mode by default but we're doing it explicitly",
   "id": "f6f362d3c428d92e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.889038Z",
     "start_time": "2024-10-21T21:15:45.883325Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.BCELoss() # binary cross entropy -- should probably ask why this specific loss function?",
   "id": "49dae3fcf107a6c3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.965489Z",
     "start_time": "2024-10-21T21:15:45.955218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ],
   "id": "c42132f3e272208b",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:45.997705Z",
     "start_time": "2024-10-21T21:15:45.989549Z"
    }
   },
   "cell_type": "code",
   "source": "fixed_noise = torch.randn(64, channels_noise, 1, 1).to(device)",
   "id": "1cf6a8e05dcfa525",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:15:46.048754Z",
     "start_time": "2024-10-21T21:15:46.042771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.utils as vutils"
   ],
   "id": "159eef8860ee7471",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:22.434592Z",
     "start_time": "2024-10-21T21:15:46.081317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"starting training...\")\n",
    "num_epochs = 10\n",
    "img_idx = 0\n",
    "\n",
    "# writer_real = SummaryWriter(log_dir='runs/GAN_MNIST/log_real')\n",
    "# writer_fake = SummaryWriter(log_dir='runs/GAN_MNIST/log_fake')\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(my_dataloader):\n",
    "        data = data.to(device)\n",
    "        batch_size = data.shape[0]\n",
    "        # apparently it's important that we train the Discriminator first\n",
    "        # Train Discriminator : max log (D(x)) + log(1-D(G(z)))\n",
    "        # We send in all real images first\n",
    "        netD.zero_grad()\n",
    "        label = (torch.ones(batch_size)*0.9).to(device) # apparently 0.9 helps? Would like to look at that again\n",
    "        output = netD(data).reshape(-1)\n",
    "        \n",
    "        lossD_real = criterion(output, label)\n",
    "        D_x = output.mean().item() # for evaluation purposes, could do something similar with the other models to get \n",
    "        \n",
    "        # now we send in all fake images to the discriminator\n",
    "        noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)\n",
    "        fake = netG(noise)\n",
    "        label = (torch.ones(batch_size)*0.1).to(device) # would what to look at this again\n",
    "        \n",
    "        output = netD(fake.detach()).reshape(-1) # telling pytorch not to trace the gradients? Not exactly sure, does that mean we're not training the generator? \n",
    "        lossD_fake = criterion(output, label)\n",
    "        \n",
    "        lossD = lossD_real + lossD_fake\n",
    "        lossD.backward() # not sure what these are doing -- look this up\n",
    "        optimizerD.step() # need to ask about this\n",
    "        \n",
    "        # Train Generator: min log(1 - D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label = torch.ones(batch_size).to(device) # not multiplying by 0.9 here\n",
    "        output = netD(fake).reshape(-1) # we actually want to train the generator now? So we don't do detach?\n",
    "        lossG = criterion(output, label)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(my_dataloader)} \\\n",
    "             Loss D: {lossD:.4f}, Loss G: {lossG:.4f}, D(x): {D_x:.4f}\")\n",
    "            \n",
    "            with torch.no_grad(): # what is happening here?\n",
    "                fake = netG(fixed_noise)\n",
    "                \n",
    "                img_grid_real = torchvision.utils.make_grid(data[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                # writer_real.add_image('MNIST Real Images', img_grid_real)\n",
    "                # writer_fake.add_image('MNIST Fake Images', img_grid_fake)\n",
    "                \n",
    "                #since Tensorboard site is still being allowed, I'll provide a locall method of viewing the pictures\n",
    "                to_pil = ToPILImage()\n",
    "                \n",
    "                # img_real = to_pil(img_grid_real)\n",
    "                # img_real.save(f'../images/experiment2/b/real/real_images_grid_{img_idx}.png')\n",
    "                \n",
    "                img_fake = to_pil(img_grid_fake)\n",
    "                img_fake.save(f'../images/experiment2/fake/b/fake_images_grid_{img_idx}.png')\n",
    "                img_idx += 1\n",
    "        "
   ],
   "id": "4e8594e0da17b164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch [0/10] Batch 0/1875              Loss D: 1.4599, Loss G: 0.8606, D(x): 0.4847\n",
      "Epoch [0/10] Batch 100/1875              Loss D: 0.6678, Loss G: 2.6850, D(x): 0.8668\n",
      "Epoch [0/10] Batch 200/1875              Loss D: 0.6645, Loss G: 2.2648, D(x): 0.8647\n",
      "Epoch [0/10] Batch 300/1875              Loss D: 0.6773, Loss G: 2.6269, D(x): 0.8957\n",
      "Epoch [0/10] Batch 400/1875              Loss D: 0.7908, Loss G: 1.5633, D(x): 0.7645\n",
      "Epoch [0/10] Batch 500/1875              Loss D: 0.7703, Loss G: 2.9945, D(x): 0.8764\n",
      "Epoch [0/10] Batch 600/1875              Loss D: 0.7799, Loss G: 1.8117, D(x): 0.8210\n",
      "Epoch [0/10] Batch 700/1875              Loss D: 1.1504, Loss G: 2.4538, D(x): 0.9104\n",
      "Epoch [0/10] Batch 800/1875              Loss D: 0.9163, Loss G: 1.4471, D(x): 0.6020\n",
      "Epoch [0/10] Batch 900/1875              Loss D: 0.7830, Loss G: 1.7556, D(x): 0.7276\n",
      "Epoch [0/10] Batch 1000/1875              Loss D: 0.7843, Loss G: 2.4975, D(x): 0.9030\n",
      "Epoch [0/10] Batch 1100/1875              Loss D: 0.9426, Loss G: 0.8342, D(x): 0.5919\n",
      "Epoch [0/10] Batch 1200/1875              Loss D: 1.2070, Loss G: 2.7341, D(x): 0.8172\n",
      "Epoch [0/10] Batch 1300/1875              Loss D: 1.0008, Loss G: 2.5126, D(x): 0.8955\n",
      "Epoch [0/10] Batch 1400/1875              Loss D: 0.7921, Loss G: 2.7224, D(x): 0.9163\n",
      "Epoch [0/10] Batch 1500/1875              Loss D: 0.7632, Loss G: 1.8335, D(x): 0.7795\n",
      "Epoch [0/10] Batch 1600/1875              Loss D: 0.7571, Loss G: 2.3776, D(x): 0.8519\n",
      "Epoch [0/10] Batch 1700/1875              Loss D: 0.8351, Loss G: 1.5378, D(x): 0.7234\n",
      "Epoch [0/10] Batch 1800/1875              Loss D: 0.7809, Loss G: 2.1737, D(x): 0.7501\n",
      "Epoch [1/10] Batch 0/1875              Loss D: 0.8582, Loss G: 2.2438, D(x): 0.8758\n",
      "Epoch [1/10] Batch 100/1875              Loss D: 0.7166, Loss G: 2.2854, D(x): 0.7988\n",
      "Epoch [1/10] Batch 200/1875              Loss D: 0.7812, Loss G: 2.1726, D(x): 0.7418\n",
      "Epoch [1/10] Batch 300/1875              Loss D: 1.0313, Loss G: 2.7571, D(x): 0.9089\n",
      "Epoch [1/10] Batch 400/1875              Loss D: 0.7774, Loss G: 1.9056, D(x): 0.7256\n",
      "Epoch [1/10] Batch 500/1875              Loss D: 0.7999, Loss G: 1.4494, D(x): 0.7051\n",
      "Epoch [1/10] Batch 600/1875              Loss D: 0.7181, Loss G: 1.7182, D(x): 0.8197\n",
      "Epoch [1/10] Batch 700/1875              Loss D: 0.7397, Loss G: 1.7340, D(x): 0.7798\n",
      "Epoch [1/10] Batch 800/1875              Loss D: 0.8259, Loss G: 1.3619, D(x): 0.7434\n",
      "Epoch [1/10] Batch 900/1875              Loss D: 0.8292, Loss G: 1.6700, D(x): 0.8030\n",
      "Epoch [1/10] Batch 1000/1875              Loss D: 0.7218, Loss G: 2.2103, D(x): 0.8518\n",
      "Epoch [1/10] Batch 1100/1875              Loss D: 0.9105, Loss G: 2.2601, D(x): 0.6173\n",
      "Epoch [1/10] Batch 1200/1875              Loss D: 0.7527, Loss G: 1.6934, D(x): 0.7751\n",
      "Epoch [1/10] Batch 1300/1875              Loss D: 0.8376, Loss G: 1.8898, D(x): 0.8218\n",
      "Epoch [1/10] Batch 1400/1875              Loss D: 0.7304, Loss G: 2.4018, D(x): 0.8168\n",
      "Epoch [1/10] Batch 1500/1875              Loss D: 0.8352, Loss G: 1.5021, D(x): 0.6715\n",
      "Epoch [1/10] Batch 1600/1875              Loss D: 0.9110, Loss G: 1.6911, D(x): 0.7374\n",
      "Epoch [1/10] Batch 1700/1875              Loss D: 0.8069, Loss G: 1.8074, D(x): 0.7952\n",
      "Epoch [1/10] Batch 1800/1875              Loss D: 0.6942, Loss G: 2.0070, D(x): 0.8536\n",
      "Epoch [2/10] Batch 0/1875              Loss D: 1.0527, Loss G: 1.0780, D(x): 0.5877\n",
      "Epoch [2/10] Batch 100/1875              Loss D: 1.0053, Loss G: 1.2097, D(x): 0.5463\n",
      "Epoch [2/10] Batch 200/1875              Loss D: 0.7383, Loss G: 1.8518, D(x): 0.8108\n",
      "Epoch [2/10] Batch 300/1875              Loss D: 0.8184, Loss G: 1.9334, D(x): 0.6825\n",
      "Epoch [2/10] Batch 400/1875              Loss D: 0.8764, Loss G: 1.5877, D(x): 0.6329\n",
      "Epoch [2/10] Batch 500/1875              Loss D: 0.7993, Loss G: 1.5359, D(x): 0.7735\n",
      "Epoch [2/10] Batch 600/1875              Loss D: 0.7792, Loss G: 1.3708, D(x): 0.7208\n",
      "Epoch [2/10] Batch 700/1875              Loss D: 0.7939, Loss G: 1.7136, D(x): 0.7640\n",
      "Epoch [2/10] Batch 800/1875              Loss D: 0.7680, Loss G: 2.2383, D(x): 0.8567\n",
      "Epoch [2/10] Batch 900/1875              Loss D: 0.7185, Loss G: 2.2815, D(x): 0.8051\n",
      "Epoch [2/10] Batch 1000/1875              Loss D: 0.8390, Loss G: 2.0580, D(x): 0.7942\n",
      "Epoch [2/10] Batch 1100/1875              Loss D: 0.7512, Loss G: 1.4853, D(x): 0.7531\n",
      "Epoch [2/10] Batch 1200/1875              Loss D: 0.9095, Loss G: 1.4567, D(x): 0.6135\n",
      "Epoch [2/10] Batch 1300/1875              Loss D: 0.7635, Loss G: 1.9798, D(x): 0.7866\n",
      "Epoch [2/10] Batch 1400/1875              Loss D: 0.7155, Loss G: 2.5150, D(x): 0.9057\n",
      "Epoch [2/10] Batch 1500/1875              Loss D: 0.7177, Loss G: 1.9511, D(x): 0.7937\n",
      "Epoch [2/10] Batch 1600/1875              Loss D: 0.9498, Loss G: 2.3580, D(x): 0.8967\n",
      "Epoch [2/10] Batch 1700/1875              Loss D: 0.8859, Loss G: 2.1815, D(x): 0.8659\n",
      "Epoch [2/10] Batch 1800/1875              Loss D: 1.1139, Loss G: 0.8317, D(x): 0.4757\n",
      "Epoch [3/10] Batch 0/1875              Loss D: 0.8438, Loss G: 2.1985, D(x): 0.8784\n",
      "Epoch [3/10] Batch 100/1875              Loss D: 0.7135, Loss G: 2.6218, D(x): 0.9108\n",
      "Epoch [3/10] Batch 200/1875              Loss D: 0.7268, Loss G: 1.7145, D(x): 0.7854\n",
      "Epoch [3/10] Batch 300/1875              Loss D: 0.8809, Loss G: 1.1483, D(x): 0.6395\n",
      "Epoch [3/10] Batch 400/1875              Loss D: 0.7170, Loss G: 1.9199, D(x): 0.8535\n",
      "Epoch [3/10] Batch 500/1875              Loss D: 0.7526, Loss G: 2.1709, D(x): 0.7684\n",
      "Epoch [3/10] Batch 600/1875              Loss D: 0.8895, Loss G: 2.2284, D(x): 0.6537\n",
      "Epoch [3/10] Batch 700/1875              Loss D: 0.7305, Loss G: 1.8757, D(x): 0.8333\n",
      "Epoch [3/10] Batch 800/1875              Loss D: 0.7493, Loss G: 2.3942, D(x): 0.8434\n",
      "Epoch [3/10] Batch 900/1875              Loss D: 0.7529, Loss G: 2.0280, D(x): 0.8360\n",
      "Epoch [3/10] Batch 1000/1875              Loss D: 0.6787, Loss G: 2.7337, D(x): 0.8870\n",
      "Epoch [3/10] Batch 1100/1875              Loss D: 0.7422, Loss G: 1.7317, D(x): 0.8095\n",
      "Epoch [3/10] Batch 1200/1875              Loss D: 0.7273, Loss G: 1.7719, D(x): 0.7891\n",
      "Epoch [3/10] Batch 1300/1875              Loss D: 0.6876, Loss G: 1.8409, D(x): 0.8353\n",
      "Epoch [3/10] Batch 1400/1875              Loss D: 1.0252, Loss G: 1.2676, D(x): 0.5368\n",
      "Epoch [3/10] Batch 1500/1875              Loss D: 0.7356, Loss G: 2.1738, D(x): 0.8723\n",
      "Epoch [3/10] Batch 1600/1875              Loss D: 0.7503, Loss G: 1.8301, D(x): 0.8106\n",
      "Epoch [3/10] Batch 1700/1875              Loss D: 0.6901, Loss G: 2.3014, D(x): 0.9214\n",
      "Epoch [3/10] Batch 1800/1875              Loss D: 0.7123, Loss G: 2.6313, D(x): 0.8907\n",
      "Epoch [4/10] Batch 0/1875              Loss D: 0.8365, Loss G: 1.3341, D(x): 0.6717\n",
      "Epoch [4/10] Batch 100/1875              Loss D: 0.6749, Loss G: 2.6362, D(x): 0.8775\n",
      "Epoch [4/10] Batch 200/1875              Loss D: 0.8103, Loss G: 2.2598, D(x): 0.8522\n",
      "Epoch [4/10] Batch 300/1875              Loss D: 0.7583, Loss G: 2.1090, D(x): 0.7520\n",
      "Epoch [4/10] Batch 400/1875              Loss D: 0.9102, Loss G: 1.8739, D(x): 0.6455\n",
      "Epoch [4/10] Batch 500/1875              Loss D: 0.7639, Loss G: 1.9466, D(x): 0.7930\n",
      "Epoch [4/10] Batch 600/1875              Loss D: 0.7528, Loss G: 1.7799, D(x): 0.7518\n",
      "Epoch [4/10] Batch 700/1875              Loss D: 0.8236, Loss G: 1.1515, D(x): 0.6765\n",
      "Epoch [4/10] Batch 800/1875              Loss D: 0.8358, Loss G: 2.1623, D(x): 0.9188\n",
      "Epoch [4/10] Batch 900/1875              Loss D: 0.7553, Loss G: 1.4190, D(x): 0.7465\n",
      "Epoch [4/10] Batch 1000/1875              Loss D: 0.7171, Loss G: 2.5306, D(x): 0.9112\n",
      "Epoch [4/10] Batch 1100/1875              Loss D: 0.7035, Loss G: 2.3371, D(x): 0.8165\n",
      "Epoch [4/10] Batch 1200/1875              Loss D: 0.7664, Loss G: 2.0058, D(x): 0.7581\n",
      "Epoch [4/10] Batch 1300/1875              Loss D: 0.6884, Loss G: 2.1424, D(x): 0.8826\n",
      "Epoch [4/10] Batch 1400/1875              Loss D: 0.6986, Loss G: 1.9792, D(x): 0.8652\n",
      "Epoch [4/10] Batch 1500/1875              Loss D: 0.7587, Loss G: 2.8641, D(x): 0.9034\n",
      "Epoch [4/10] Batch 1600/1875              Loss D: 0.8123, Loss G: 1.6202, D(x): 0.7773\n",
      "Epoch [4/10] Batch 1700/1875              Loss D: 0.7084, Loss G: 2.2064, D(x): 0.8062\n",
      "Epoch [4/10] Batch 1800/1875              Loss D: 0.7705, Loss G: 2.0571, D(x): 0.9056\n",
      "Epoch [5/10] Batch 0/1875              Loss D: 0.6747, Loss G: 2.5329, D(x): 0.8955\n",
      "Epoch [5/10] Batch 100/1875              Loss D: 0.7698, Loss G: 1.5033, D(x): 0.7325\n",
      "Epoch [5/10] Batch 200/1875              Loss D: 0.7156, Loss G: 1.6745, D(x): 0.8261\n",
      "Epoch [5/10] Batch 300/1875              Loss D: 0.7054, Loss G: 2.1698, D(x): 0.8451\n",
      "Epoch [5/10] Batch 400/1875              Loss D: 0.7330, Loss G: 2.8466, D(x): 0.9052\n",
      "Epoch [5/10] Batch 500/1875              Loss D: 0.7107, Loss G: 1.8879, D(x): 0.7962\n",
      "Epoch [5/10] Batch 600/1875              Loss D: 0.6739, Loss G: 2.1693, D(x): 0.9009\n",
      "Epoch [5/10] Batch 700/1875              Loss D: 0.6875, Loss G: 2.3606, D(x): 0.8670\n",
      "Epoch [5/10] Batch 800/1875              Loss D: 0.8064, Loss G: 2.4075, D(x): 0.9200\n",
      "Epoch [5/10] Batch 900/1875              Loss D: 0.7235, Loss G: 2.2186, D(x): 0.8947\n",
      "Epoch [5/10] Batch 1000/1875              Loss D: 0.8641, Loss G: 3.2473, D(x): 0.9249\n",
      "Epoch [5/10] Batch 1100/1875              Loss D: 0.6869, Loss G: 2.8565, D(x): 0.9188\n",
      "Epoch [5/10] Batch 1200/1875              Loss D: 0.7076, Loss G: 2.4197, D(x): 0.9075\n",
      "Epoch [5/10] Batch 1300/1875              Loss D: 0.8314, Loss G: 1.0384, D(x): 0.6763\n",
      "Epoch [5/10] Batch 1400/1875              Loss D: 0.8303, Loss G: 2.7309, D(x): 0.8880\n",
      "Epoch [5/10] Batch 1500/1875              Loss D: 0.7024, Loss G: 2.3874, D(x): 0.8334\n",
      "Epoch [5/10] Batch 1600/1875              Loss D: 0.8646, Loss G: 2.4820, D(x): 0.8620\n",
      "Epoch [5/10] Batch 1700/1875              Loss D: 0.6843, Loss G: 2.2904, D(x): 0.8358\n",
      "Epoch [5/10] Batch 1800/1875              Loss D: 0.7722, Loss G: 2.3927, D(x): 0.8949\n",
      "Epoch [6/10] Batch 0/1875              Loss D: 0.7116, Loss G: 2.4509, D(x): 0.9161\n",
      "Epoch [6/10] Batch 100/1875              Loss D: 0.7103, Loss G: 2.2211, D(x): 0.8403\n",
      "Epoch [6/10] Batch 200/1875              Loss D: 0.6913, Loss G: 2.3075, D(x): 0.8610\n",
      "Epoch [6/10] Batch 300/1875              Loss D: 0.7461, Loss G: 2.0590, D(x): 0.8007\n",
      "Epoch [6/10] Batch 400/1875              Loss D: 0.7291, Loss G: 2.2193, D(x): 0.9093\n",
      "Epoch [6/10] Batch 500/1875              Loss D: 0.7394, Loss G: 2.5643, D(x): 0.8851\n",
      "Epoch [6/10] Batch 600/1875              Loss D: 0.7291, Loss G: 1.5488, D(x): 0.7797\n",
      "Epoch [6/10] Batch 700/1875              Loss D: 0.6778, Loss G: 2.8237, D(x): 0.9183\n",
      "Epoch [6/10] Batch 800/1875              Loss D: 0.6770, Loss G: 2.2809, D(x): 0.8843\n",
      "Epoch [6/10] Batch 900/1875              Loss D: 0.6868, Loss G: 2.0773, D(x): 0.8322\n",
      "Epoch [6/10] Batch 1000/1875              Loss D: 0.8206, Loss G: 1.5421, D(x): 0.7045\n",
      "Epoch [6/10] Batch 1100/1875              Loss D: 0.6999, Loss G: 2.2817, D(x): 0.8623\n",
      "Epoch [6/10] Batch 1200/1875              Loss D: 0.7744, Loss G: 1.7886, D(x): 0.7547\n",
      "Epoch [6/10] Batch 1300/1875              Loss D: 0.8714, Loss G: 1.9142, D(x): 0.7374\n",
      "Epoch [6/10] Batch 1400/1875              Loss D: 0.7542, Loss G: 1.7935, D(x): 0.7723\n",
      "Epoch [6/10] Batch 1500/1875              Loss D: 0.6852, Loss G: 2.3267, D(x): 0.8453\n",
      "Epoch [6/10] Batch 1600/1875              Loss D: 0.7791, Loss G: 2.4414, D(x): 0.7167\n",
      "Epoch [6/10] Batch 1700/1875              Loss D: 0.7598, Loss G: 2.2421, D(x): 0.9403\n",
      "Epoch [6/10] Batch 1800/1875              Loss D: 0.7033, Loss G: 2.1687, D(x): 0.9013\n",
      "Epoch [7/10] Batch 0/1875              Loss D: 0.6967, Loss G: 2.0632, D(x): 0.8324\n",
      "Epoch [7/10] Batch 100/1875              Loss D: 0.7590, Loss G: 2.2877, D(x): 0.7710\n",
      "Epoch [7/10] Batch 200/1875              Loss D: 0.6773, Loss G: 1.8899, D(x): 0.8654\n",
      "Epoch [7/10] Batch 300/1875              Loss D: 0.6947, Loss G: 3.0098, D(x): 0.9248\n",
      "Epoch [7/10] Batch 400/1875              Loss D: 0.6972, Loss G: 2.2544, D(x): 0.8323\n",
      "Epoch [7/10] Batch 500/1875              Loss D: 0.6769, Loss G: 2.3245, D(x): 0.9040\n",
      "Epoch [7/10] Batch 600/1875              Loss D: 0.6728, Loss G: 1.9771, D(x): 0.8775\n",
      "Epoch [7/10] Batch 700/1875              Loss D: 0.6976, Loss G: 2.2056, D(x): 0.9022\n",
      "Epoch [7/10] Batch 800/1875              Loss D: 0.6798, Loss G: 2.8459, D(x): 0.8978\n",
      "Epoch [7/10] Batch 900/1875              Loss D: 0.6809, Loss G: 1.9261, D(x): 0.8422\n",
      "Epoch [7/10] Batch 1000/1875              Loss D: 0.6733, Loss G: 2.1308, D(x): 0.8847\n",
      "Epoch [7/10] Batch 1100/1875              Loss D: 0.6895, Loss G: 2.3951, D(x): 0.8380\n",
      "Epoch [7/10] Batch 1200/1875              Loss D: 0.8524, Loss G: 2.3749, D(x): 0.8506\n",
      "Epoch [7/10] Batch 1300/1875              Loss D: 0.7069, Loss G: 1.8859, D(x): 0.8124\n",
      "Epoch [7/10] Batch 1400/1875              Loss D: 0.7138, Loss G: 1.8574, D(x): 0.7973\n",
      "Epoch [7/10] Batch 1500/1875              Loss D: 0.6987, Loss G: 2.6871, D(x): 0.9212\n",
      "Epoch [7/10] Batch 1600/1875              Loss D: 0.6741, Loss G: 2.3992, D(x): 0.8917\n",
      "Epoch [7/10] Batch 1700/1875              Loss D: 0.6862, Loss G: 2.1665, D(x): 0.8470\n",
      "Epoch [7/10] Batch 1800/1875              Loss D: 0.7908, Loss G: 1.7781, D(x): 0.8170\n",
      "Epoch [8/10] Batch 0/1875              Loss D: 0.8092, Loss G: 2.3589, D(x): 0.8640\n",
      "Epoch [8/10] Batch 100/1875              Loss D: 0.6791, Loss G: 1.9090, D(x): 0.8690\n",
      "Epoch [8/10] Batch 200/1875              Loss D: 0.6751, Loss G: 2.2896, D(x): 0.8685\n",
      "Epoch [8/10] Batch 300/1875              Loss D: 0.8887, Loss G: 1.4439, D(x): 0.6377\n",
      "Epoch [8/10] Batch 400/1875              Loss D: 0.6839, Loss G: 2.6991, D(x): 0.9028\n",
      "Epoch [8/10] Batch 500/1875              Loss D: 0.7178, Loss G: 1.5658, D(x): 0.7864\n",
      "Epoch [8/10] Batch 600/1875              Loss D: 0.6935, Loss G: 2.4408, D(x): 0.9373\n",
      "Epoch [8/10] Batch 700/1875              Loss D: 0.6875, Loss G: 1.8671, D(x): 0.8281\n",
      "Epoch [8/10] Batch 800/1875              Loss D: 0.6666, Loss G: 2.2561, D(x): 0.8953\n",
      "Epoch [8/10] Batch 900/1875              Loss D: 0.6894, Loss G: 2.2948, D(x): 0.8520\n",
      "Epoch [8/10] Batch 1000/1875              Loss D: 0.6911, Loss G: 1.9564, D(x): 0.8353\n",
      "Epoch [8/10] Batch 1100/1875              Loss D: 0.6712, Loss G: 2.4877, D(x): 0.9105\n",
      "Epoch [8/10] Batch 1200/1875              Loss D: 1.2502, Loss G: 0.6227, D(x): 0.4365\n",
      "Epoch [8/10] Batch 1300/1875              Loss D: 0.7238, Loss G: 1.7789, D(x): 0.8283\n",
      "Epoch [8/10] Batch 1400/1875              Loss D: 0.6798, Loss G: 2.7788, D(x): 0.9162\n",
      "Epoch [8/10] Batch 1500/1875              Loss D: 0.7242, Loss G: 2.0819, D(x): 0.8590\n",
      "Epoch [8/10] Batch 1600/1875              Loss D: 0.6669, Loss G: 2.3015, D(x): 0.9129\n",
      "Epoch [8/10] Batch 1700/1875              Loss D: 0.6904, Loss G: 1.9146, D(x): 0.8575\n",
      "Epoch [8/10] Batch 1800/1875              Loss D: 0.7646, Loss G: 1.4911, D(x): 0.7252\n",
      "Epoch [9/10] Batch 0/1875              Loss D: 0.6693, Loss G: 2.5250, D(x): 0.8630\n",
      "Epoch [9/10] Batch 100/1875              Loss D: 0.7012, Loss G: 2.4128, D(x): 0.9222\n",
      "Epoch [9/10] Batch 200/1875              Loss D: 0.6748, Loss G: 2.4583, D(x): 0.9190\n",
      "Epoch [9/10] Batch 300/1875              Loss D: 0.7009, Loss G: 2.3255, D(x): 0.9060\n",
      "Epoch [9/10] Batch 400/1875              Loss D: 0.6651, Loss G: 2.2759, D(x): 0.8812\n",
      "Epoch [9/10] Batch 500/1875              Loss D: 1.3442, Loss G: 1.2513, D(x): 0.3956\n",
      "Epoch [9/10] Batch 600/1875              Loss D: 0.6999, Loss G: 2.0284, D(x): 0.8099\n",
      "Epoch [9/10] Batch 700/1875              Loss D: 0.7206, Loss G: 2.2973, D(x): 0.7963\n",
      "Epoch [9/10] Batch 800/1875              Loss D: 0.6688, Loss G: 2.3501, D(x): 0.9086\n",
      "Epoch [9/10] Batch 900/1875              Loss D: 0.6697, Loss G: 1.7738, D(x): 0.8655\n",
      "Epoch [9/10] Batch 1000/1875              Loss D: 0.6822, Loss G: 2.1359, D(x): 0.8305\n",
      "Epoch [9/10] Batch 1100/1875              Loss D: 0.6686, Loss G: 2.2413, D(x): 0.9227\n",
      "Epoch [9/10] Batch 1200/1875              Loss D: 0.6710, Loss G: 2.5726, D(x): 0.9025\n",
      "Epoch [9/10] Batch 1300/1875              Loss D: 0.6652, Loss G: 2.3082, D(x): 0.8789\n",
      "Epoch [9/10] Batch 1400/1875              Loss D: 0.7061, Loss G: 1.9088, D(x): 0.8137\n",
      "Epoch [9/10] Batch 1500/1875              Loss D: 0.6880, Loss G: 2.6305, D(x): 0.9203\n",
      "Epoch [9/10] Batch 1600/1875              Loss D: 1.5546, Loss G: 0.6021, D(x): 0.3080\n",
      "Epoch [9/10] Batch 1700/1875              Loss D: 0.6894, Loss G: 2.4199, D(x): 0.9131\n",
      "Epoch [9/10] Batch 1800/1875              Loss D: 0.7088, Loss G: 1.8281, D(x): 0.8024\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:23.362826Z",
     "start_time": "2024-10-21T21:32:22.571740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained Inception v3 model\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).eval().to(device)"
   ],
   "id": "b4bbdb6199610e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meddi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\meddi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:23.928020Z",
     "start_time": "2024-10-21T21:32:23.911560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_inception_score(rgb_images, inception_model, splits=10):\n",
    "    # rgb_images = images.repeat(1, 3, 1, 1) # not necessary since I'm doing that before\n",
    "    \n",
    "    rgb_images_resized = F.interpolate(rgb_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = inception_model(rgb_images_resized).softmax(dim=1)\n",
    "        \n",
    "    split_scores = []\n",
    "    for i in range(splits):\n",
    "        part = preds[i * (len(preds) // splits): ((i + 1) * len(preds) // splits), :]\n",
    "        p_y = part.mean(dim=0)\n",
    "        split_scores.append(torch.exp((part * (part.log() - p_y.log())).sum(dim=1).mean()))\n",
    "        \n",
    "    return torch.mean(torch.tensor(split_scores)), torch.std(torch.tensor(split_scores))"
   ],
   "id": "35da5b642c9af803",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:24.295126Z",
     "start_time": "2024-10-21T21:32:24.274439Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "ba2f88762984cfb0",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:24.647504Z",
     "start_time": "2024-10-21T21:32:24.634300Z"
    }
   },
   "cell_type": "code",
   "source": "from scipy import linalg",
   "id": "42ee06839d178a5e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:25.143501Z",
     "start_time": "2024-10-21T21:32:24.980970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_images = 32\n",
    "\n",
    "first_batch = next(iter(my_dataloader))\n",
    "\n",
    "real_images = first_batch[0]\n",
    "real_images = real_images[:num_images]\n",
    "real_images = (real_images + 1) / 2\n",
    "rgb_real_images = real_images.repeat(1, 3, 1, 1)\n",
    "rgb_real_images = rgb_real_images.to(device)\n",
    "print(\"rgb_real_images shape: \", rgb_real_images.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(num_images, channels_noise, 1, 1).to(device)\n",
    "    fake_images = netG(noise).to(device)\n",
    "\n",
    "    fake_images = (fake_images + 1) / 2\n",
    "    rgb_fake_images = fake_images.repeat(1, 3, 1, 1)\n",
    "    \n",
    "print(\"rgb_fake_images shape, \", rgb_fake_images.shape)\n",
    "    "
   ],
   "id": "46e50507c0b0e18d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_real_images shape:  torch.Size([32, 3, 64, 64])\n",
      "rgb_fake_images shape,  torch.Size([32, 3, 64, 64])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:25.671791Z",
     "start_time": "2024-10-21T21:32:25.656087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_activation_statistics(images, model, dims=2048, batch_size=128):\n",
    "    model.eval()\n",
    "    act = np.empty((len(images), dims))\n",
    "    \n",
    "    batch = images.cuda()\n",
    "    \n",
    "    pred = model(batch)[0]\n",
    "    \n",
    "    # if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "    #     # pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "    #     print(\"triggered weird check I don't have a function for\")\n",
    "        \n",
    "    act = pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "    \n",
    "    "
   ],
   "id": "4cb13ae16d2f658",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:26.186234Z",
     "start_time": "2024-10-21T21:32:26.171368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    \n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different legnths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # does something similar where it checks to see if the covmean is valid and same as the positive semi definite\n",
    "    # it adds an epsilon, though at a different point\n",
    "    if not np.isinf(covmean).all():\n",
    "        print(\"frechet distance failed; adding to diagonal of cov estimates\")\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        \n",
    "        covmean, _ = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset), disp=False)\n",
    "        \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "        \n",
    "    tr_covmean = np.trace(covmean)\n",
    "    fid = (diff.dot(diff)) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    return fid"
   ],
   "id": "8387c7ccae11f38b",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:26.558292Z",
     "start_time": "2024-10-21T21:32:26.549605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_fid_score(real_images, fake_images, model):\n",
    "    mu1, sigma1 = calculate_activation_statistics(real_images, model)\n",
    "    mu2, sigma2 = calculate_activation_statistics(fake_images, model)\n",
    "    \n",
    "    fid_value = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return fid_value"
   ],
   "id": "e3f25031b52e29b1",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:26.868405Z",
     "start_time": "2024-10-21T21:32:26.859111Z"
    }
   },
   "cell_type": "code",
   "source": "rgb_real_images.shape",
   "id": "b6849da4f357182b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:27.139250Z",
     "start_time": "2024-10-21T21:32:27.131836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resize_images(images):\n",
    "    return F.interpolate(images, size=(299,  299), mode='bilinear', align_corners=False)"
   ],
   "id": "855a89319157d985",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:27.438134Z",
     "start_time": "2024-10-21T21:32:27.414058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resized_rgb_real_images = resize_images(rgb_real_images)\n",
    "resized_rgb_fake_images = resize_images(rgb_fake_images)\n",
    "\n",
    "print(\"resized rgb_real_images shape: \", resized_rgb_real_images.shape)\n",
    "print(\"resized rgb_fake_images shape: \", resized_rgb_fake_images.shape)"
   ],
   "id": "308d4a29ba9b26bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resized rgb_real_images shape:  torch.Size([32, 3, 299, 299])\n",
      "resized rgb_fake_images shape:  torch.Size([32, 3, 299, 299])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:41.333550Z",
     "start_time": "2024-10-21T21:32:27.884023Z"
    }
   },
   "cell_type": "code",
   "source": "fid_score = calculate_fid_score(resized_rgb_real_images, resized_rgb_fake_images, inception_model)",
   "id": "cd7ed259365cb589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frechet distance failed; adding to diagonal of cov estimates\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:41.540276Z",
     "start_time": "2024-10-21T21:32:41.533848Z"
    }
   },
   "cell_type": "code",
   "source": "fid_score",
   "id": "b6b62520245676ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006938047254569923"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:48.743891Z",
     "start_time": "2024-10-21T21:32:41.880674Z"
    }
   },
   "cell_type": "code",
   "source": "inception_score, is_std = calculate_inception_score(rgb_fake_images, inception_model)",
   "id": "d4db4e7892df46d2",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:49.013447Z",
     "start_time": "2024-10-21T21:32:49.004619Z"
    }
   },
   "cell_type": "code",
   "source": "inception_score",
   "id": "1aa1039d6382edfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6273)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:49.672268Z",
     "start_time": "2024-10-21T21:32:49.662600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np_is_score = inception_score.numpy()\n",
    "\n",
    "\n",
    "with open('base_dcgan', \"a\") as file:\n",
    "    file.write(\"Base_dcgan measurements\\n\")\n",
    "    file.write(f\"inception score {np.array2string(np_is_score)}\\n\")\n",
    "    file.write(f\"fid score {np.array2string(fid_score)}\\n\")\n"
   ],
   "id": "f7700e050ed21e7f",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:32:49.696674Z",
     "start_time": "2024-10-21T21:32:49.692323Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8b9609df4de296f7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
